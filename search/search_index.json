{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Cactus","text":"<p>A hybrid low-latency energy-efficient AI engine for mobile devices &amp; wearables.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Cactus Engine  \u2502 \u2190\u2500\u2500 OpenAI-compatible APIs for all major languages\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     Chat, vision, STT, RAG, tool call, cloud handoff\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Cactus Graph   \u2502 \u2190\u2500\u2500 Zero-copy computation graph (PyTorch for mobile)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     Custom models, optimised for RAM &amp; quantisation\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Cactus Kernels  \u2502 \u2190\u2500\u2500 ARM SIMD kernels (Apple, Snapdragon, Exynos, etc)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     Custom attention, KV-cache quant, chunked prefill\n</code></pre>"},{"location":"#quick-demo","title":"Quick Demo","text":"<ul> <li>Step 1: <code>brew install cactus-compute/cactus/cactus</code></li> <li>Step 2: <code>cactus transcribe</code> or <code>cactus run</code> </li> </ul>"},{"location":"#cactus-engine","title":"Cactus Engine","text":"<p><pre><code>#include cactus.h\n\ncactus_model_t model = cactus_init(\n    \"path/to/weight/folder\",\n    \"path to txt or dir of txts for auto-rag\",\n);\n\nconst char* messages = R\"([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"My name is Henry Ndubuaku\"}\n])\";\n\nconst char* options = R\"({\n    \"max_tokens\": 50,\n    \"stop_sequences\": [\"&lt;|im_end|&gt;\"]\n})\";\n\nchar response[4096];\nint result = cactus_complete(\n    model,            // model handle\n    messages,         // JSON chat messages\n    response,         // response buffer\n    sizeof(response), // buffer size\n    options,          // generation options\n    nullptr,          // tools JSON\n    nullptr,          // streaming callback\n    nullptr           // user data\n);\n</code></pre> Example response from Gemma3-270m <pre><code>{\n    \"success\": true,        // generation succeeded\n    \"error\": null,          // error details if failed\n    \"cloud_handoff\": false, // true if cloud model used\n    \"response\": \"Hi there!\",\n    \"function_calls\": [],   // parsed tool calls\n    \"confidence\": 0.8193,   // model confidence\n    \"time_to_first_token_ms\": 45.23,\n    \"total_time_ms\": 163.67,\n    \"prefill_tps\": 1621.89,\n    \"decode_tps\": 168.42,\n    \"ram_usage_mb\": 245.67,\n    \"prefill_tokens\": 28,\n    \"decode_tokens\": 50,\n    \"total_tokens\": 78\n}\n</code></pre></p>"},{"location":"#cactus-graph","title":"Cactus Graph","text":"<pre><code>#include cactus.h\n\nCactusGraph graph;\nauto a = graph.input({2, 3}, Precision::FP16);\nauto b = graph.input({3, 4}, Precision::INT8);\n\nauto x1 = graph.matmul(a, b, false);\nauto x2 = graph.transpose(x1);\nauto result = graph.matmul(b, x2, true);\n\nfloat a_data[6] = {1.1f, 2.3f, 3.4f, 4.2f, 5.7f, 6.8f};\nfloat b_data[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};\n\ngraph.set_input(a, a_data, Precision::FP16);\ngraph.set_input(b, b_data, Precision::INT8);\n\ngraph.execute();\nvoid* output_data = graph.get_output(result);\n\ngraph.hard_reset(); \n</code></pre>"},{"location":"#api-sdk-references","title":"API &amp; SDK References","text":"Reference Language Description Engine API C Chat completion, streaming, tool calling, transcription, embeddings, RAG, vision, VAD, vector index, cloud handoff Graph API C++ Tensor operations, matrix multiplication, attention, normalization, activation functions Python SDK Python Mac, Linux Swift SDK Swift iOS, macOS, tvOS, watchOS, Android Kotlin SDK Kotlin Android, iOS (via KMP) Flutter SDK Dart iOS, macOS, Android Rust SDK Rust Mac, Linux React Native JavaScript iOS, Android"},{"location":"#benchmarks","title":"Benchmarks","text":"<ul> <li>All weights INT4 quantised</li> <li>LFM: 1k-prefill / 100-decode, values are prefill tps / decode tps</li> <li>LFM-VL: 256px input, values are latency / decode tps</li> <li>Parakeet: 30s audio input, values are latency / decode tps</li> <li>Missing latency = no NPU support yet</li> </ul> Device LFM 1.2B LFMVL 1.6B Parakeet 1.1B RAM Mac M4 Pro 582/100 0.2s/98 0.1s/900k+ 76MB iPad/Mac M3 350/60 0.3s/69 0.3s/800k+ 70MB iPhone 17 Pro 327/48 0.3s/48 0.3s/300k+ 108MB iPhone 13 Mini 148/34 0.3s/35 0.7s/90k+ 1GB Galaxy S25 Ultra 255/37 -/34 -/250k+ 1.5GB Pixel 6a 70/15 -/15 -/17k+ 1GB Galaxy A17 5G 32/10 -/11 -/40k+ 727MB CMF Phone 2 Pro - - - - Raspberry Pi 5 69/11 13.3s/11 4.5s/180k+ 869MB"},{"location":"#roadmap","title":"Roadmap","text":"Date Status Milestone Sep 2025 Done Released v1 Oct 2025 Done Chunked prefill, KVCache Quant (2x prefill) Nov 2025 Done Cactus Attention (10 &amp; 1k prefill = same decode) Dec 2025 Done Team grows to +6 Research Engineers Jan 2026 Done Apple NPU/RAM, 5-11x faster iOS/Mac Feb 2026 Done Hybrid inference, INT4, lossless Quant (1.5x) Mar 2026 Coming Qualcomm/Google NPUs, 5-11x faster Android Apr 2026 Coming Mediatek/Exynos NPUs, Cactus@ICLR May 2026 Coming Kernel\u2192C++, Graph/Engine\u2192Rust, Mac GPU &amp; VR Jun 2026 Coming Torch/JAX model transpilers Jul 2026 Coming Wearables optimisations, Cactus@ICML Aug 2026 Coming Orchestration Sep 2026 Coming Full Cactus paper, chip manufacturer partners"},{"location":"#using-this-repo","title":"Using this repo","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                              \u2502\n\u2502 Step 0: if on Linux (Ubuntu/Debian)                                          \u2502\n\u2502 sudo apt-get install python3 python3-venv python3-pip cmake                  \u2502\n\u2502   build-essential libcurl4-openssl-dev                                       \u2502\n\u2502                                                                              \u2502\n\u2502 Step 1: clone and setup                                                      \u2502\n\u2502 git clone https://github.com/cactus-compute/cactus &amp;&amp; cd cactus              \u2502\n\u2502 source ./setup                                                               \u2502\n\u2502                                                                              \u2502\n\u2502 Step 2: use the commands                                                     \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n\u2502                                                                              \u2502\n\u2502  cactus auth                         manage Cloud API key                    \u2502\n\u2502    --status                          show key status                         \u2502\n\u2502    --clear                           remove saved key                        \u2502\n\u2502                                                                              \u2502\n\u2502  cactus run &lt;model&gt;                  opens playground (auto downloads)       \u2502\n\u2502    --precision INT4|INT8|FP16        quantization (default: INT4)            \u2502\n\u2502    --token &lt;token&gt;                   HF token (gated models)                 \u2502\n\u2502    --reconvert                       force reconversion from source          \u2502\n\u2502                                                                              \u2502\n\u2502  cactus transcribe [model]           live mic transcription (parakeet-1.1b)  \u2502\n\u2502    --file &lt;audio.wav&gt;                transcribe file instead of mic          \u2502\n\u2502    --precision INT4|INT8|FP16        quantization (default: INT4)            \u2502\n\u2502    --token &lt;token&gt;                   HF token (gated models)                 \u2502\n\u2502    --reconvert                       force reconversion from source          \u2502\n\u2502                                                                              \u2502\n\u2502  cactus download &lt;model&gt;             downloads model to ./weights            \u2502\n\u2502    --precision INT4|INT8|FP16        quantization (default: INT4)            \u2502\n\u2502    --token &lt;token&gt;                   HuggingFace API token                   \u2502\n\u2502    --reconvert                       force reconversion from source          \u2502\n\u2502                                                                              \u2502\n\u2502  cactus convert &lt;model&gt; [dir]        convert model, supports LoRA merge      \u2502\n\u2502    --precision INT4|INT8|FP16        quantization (default: INT4)            \u2502\n\u2502    --lora &lt;path&gt;                     LoRA adapter to merge                   \u2502\n\u2502    --token &lt;token&gt;                   HuggingFace API token                   \u2502\n\u2502                                                                              \u2502\n\u2502  cactus build                        build for ARM \u2192 build/libcactus.a       \u2502\n\u2502    --apple                           Apple (iOS/macOS)                       \u2502\n\u2502    --android                         Android                                 \u2502\n\u2502    --flutter                         Flutter (all platforms)                 \u2502\n\u2502    --python                          shared lib for Python FFI               \u2502\n\u2502                                                                              \u2502\n\u2502  cactus test                         run unit tests and benchmarks           \u2502\n\u2502    --model &lt;model&gt;                   default: LFM2-VL-450M                   \u2502\n\u2502    --transcribe_model &lt;model&gt;        default: moonshine-base                 \u2502\n\u2502    --benchmark                       use larger models                       \u2502\n\u2502    --precision INT4|INT8|FP16        regenerate weights with precision       \u2502\n\u2502    --reconvert                       force reconversion from source          \u2502\n\u2502    --no-rebuild                      skip building library                   \u2502\n\u2502    --only &lt;test&gt;                     specific test (llm, vlm, stt, etc)      \u2502\n\u2502    --ios                             run on connected iPhone                 \u2502\n\u2502    --android                         run on connected Android                \u2502\n\u2502                                                                              \u2502\n\u2502  cactus clean                        remove all build artifacts              \u2502\n\u2502  cactus --help                       show all commands and flags             \u2502\n\u2502                                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#supported-models","title":"Supported Models","text":"Model Features google/gemma-3-270m-it completion google/functiongemma-270m-it completion, tools LiquidAI/LFM2-350M completion, tools, embed Qwen/Qwen3-0.6B completion, tools, embed LiquidAI/LFM2-700M completion, tools, embed LiquidAI/LFM2-8B-A1B completion, tools, embed google/gemma-3-1b-it completion LiquidAI/LFM2.5-1.2B-Thinking completion, tools, embed LiquidAI/LFM2.5-1.2B-Instruct completion, tools, embed Qwen/Qwen3-1.7B completion, tools, embed LiquidAI/LFM2-2.6B completion, tools, embed LiquidAI/LFM2-VL-450M vision, txt &amp; img embed, Apple NPU LiquidAI/LFM2.5-VL-1.6B vision, txt &amp; img embed, Apple NPU UsefulSensors/moonshine-base transcription, speech embed openai/whisper-small transcription, speech embed, Apple NPU openai/whisper-medium transcribe, speech embed, Apple NPU nvidia/parakeet-ctc-0.6b transcribe, speech embed, Apple NPU nvidia/parakeet-ctc-1.1b transcribe, speech embed, Apple NPU snakers4/silero-vad vad nomic-ai/nomic-embed-text-v2-moe embed Qwen/Qwen3-Embedding-0.6B embed"},{"location":"#maintaining-organisations","title":"Maintaining Organisations","text":"<ol> <li>Cactus Compute, Inc. (YC S25)</li> <li>UCLA's BruinAI</li> <li>Char (YC S25)</li> <li>Yale's AI Society</li> <li>National Unoversity of Singapore's AI Society</li> <li>UC Irvine's AI@UCI</li> <li>Imperial College's AI Society</li> <li>University of Pennsylvania's AI@Penn</li> <li>University of Michigan Ann-Arbor MSAIL</li> <li>University of Colorado Boulder's AI Club</li> </ol>"},{"location":"#citation","title":"Citation","text":"<p>If you use Cactus in your research, please cite it as follows:</p> <pre><code>@software{cactus,\n  title        = {Cactus: AI Inference Engine for Phones &amp; Wearables},\n  author       = {Ndubuaku, Henry and Cactus Team},\n  url          = {https://github.com/cactus-compute/cactus},\n  year         = {2025}\n}\n</code></pre> <p>N/B: Scroll all the way up and click the shields link for resources!</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Cactus","text":"<p>Thank you for your interest in contributing to Cactus! This document covers the guidelines and process for making contributions.</p>"},{"location":"CONTRIBUTING/#code-guidelines","title":"Code Guidelines","text":"<ul> <li>C++ Standard: Use C++20 features where appropriate.</li> <li>Formatting: Follow the existing code style in the project, one header per folder.</li> <li>Comments: Avoid comments, make your code read like plain english.</li> <li>AI-Generated Code: Do not blindly PR AI slop, this codebase is very complex, they miss details.</li> <li>Update docs: Please update docs when necessary, be intuitive and straightforward.</li> <li>Keep It Simple: Do not go beyond the scope of the GH issue, avoid bloated PRs, keep codes lean.</li> <li>Benchmark Your Changes: Test performance impact, Cactus is performance-critical.</li> <li>Test everything: A PR that fails to build is the biggest red flag, means it was not tested.</li> </ul>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository and create a branch from <code>main</code>.</li> <li>Make your changes, keeping the scope focused on the relevant GitHub issue.</li> <li>Run <code>cactus test</code> to verify your changes build and pass all tests.</li> <li>Run <code>cactus test --benchmark</code> if your changes affect performance-critical paths.</li> <li>Update documentation if your changes affect the public API or user-facing behavior.</li> <li>Submit a pull request with a clear description of what you changed and why.</li> </ol>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<pre><code># Run all tests\ncactus test\n\n# Run tests on a connected iOS device\ncactus test --ios\n\n# Run tests on a connected Android device\ncactus test --android\n\n# Run benchmarks\ncactus test --benchmark\n\n# Test a specific model\ncactus test --model LiquidAI/LFM2.5-1.2B-Instruct\n</code></pre>"},{"location":"CONTRIBUTING/#developer-certificate-of-origin","title":"Developer Certificate of Origin","text":"<p>All contributions must comply with the Developer Certificate of Origin (DCO). By submitting a contribution, you certify that you have the right to do so under the project's open source license.</p>"},{"location":"CONTRIBUTING/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 C API reference</li> <li>Cactus Graph API \u2014 Computational graph API reference</li> <li>Documentation Index \u2014 Full documentation table of contents</li> </ul>"},{"location":"android/","title":"Cactus for Android &amp; Kotlin Multiplatform","text":"<p>Run AI models on-device with a simple Kotlin API.</p>"},{"location":"android/#building","title":"Building","text":"<pre><code>cactus build --android\n</code></pre> <p>Build output: <code>android/build/lib/libcactus.so</code></p> <p>see the main README.md for how to use CLI &amp; download weight</p>"},{"location":"android/#vendored-libcurl-device-builds","title":"Vendored libcurl (device builds)","text":"<p>To bundle libcurl locally for Android device testing, place artifacts using:</p> <p><code>libs/curl/android/arm64-v8a/libcurl.a</code> and <code>libs/curl/include/curl/*.h</code></p> <p>The build auto-detects <code>libs/curl</code>. You can override with:</p> <pre><code>CACTUS_CURL_ROOT=/absolute/path/to/curl cactus build --android\n</code></pre>"},{"location":"android/#integration","title":"Integration","text":""},{"location":"android/#android-only","title":"Android-only","text":"<ol> <li>Copy <code>libcactus.so</code> to <code>app/src/main/jniLibs/arm64-v8a/</code></li> <li>Copy <code>Cactus.kt</code> to <code>app/src/main/java/com/cactus/</code></li> </ol>"},{"location":"android/#kotlin-multiplatform","title":"Kotlin Multiplatform","text":"<p>Source files:</p> File Copy to <code>Cactus.common.kt</code> <code>shared/src/commonMain/kotlin/com/cactus/</code> <code>Cactus.android.kt</code> <code>shared/src/androidMain/kotlin/com/cactus/</code> <code>Cactus.ios.kt</code> <code>shared/src/iosMain/kotlin/com/cactus/</code> <code>cactus.def</code> <code>shared/src/nativeInterop/cinterop/</code> <p>Binary files:</p> Platform Location Android <code>libcactus.so</code> \u2192 <code>app/src/main/jniLibs/arm64-v8a/</code> iOS <code>libcactus-device.a</code> \u2192 link via cinterop <p>build.gradle.kts:</p> <pre><code>kotlin {\n    androidTarget()\n\n    listOf(iosArm64(), iosSimulatorArm64()).forEach {\n        it.compilations.getByName(\"main\") {\n            cinterops {\n                create(\"cactus\") {\n                    defFile(\"src/nativeInterop/cinterop/cactus.def\")\n                    includeDirs(\"/path/to/cactus/ffi\")\n                }\n            }\n        }\n        it.binaries.framework {\n            linkerOpts(\"-L/path/to/apple\", \"-lcactus-device\")\n        }\n    }\n\n    sourceSets {\n        commonMain.dependencies {\n            implementation(\"org.jetbrains.kotlinx:kotlinx-serialization-json:1.6.0\")\n        }\n    }\n}\n</code></pre>"},{"location":"android/#usage","title":"Usage","text":""},{"location":"android/#basic-completion","title":"Basic Completion","text":"<pre><code>import com.cactus.*\n\nval model = Cactus.create(\"/path/to/model\")\nval result = model.complete(\"What is the capital of France?\")\nmodel.close()\n</code></pre>"},{"location":"android/#chat-messages","title":"Chat Messages","text":"<pre><code>Cactus.create(modelPath).use { model -&gt;\n    val result = model.complete(\n        messages = listOf(\n            Message.system(\"You are a helpful assistant.\"),\n            Message.user(\"What is 2 + 2?\")\n        )\n    )\n    println(result.text)\n}\n</code></pre>"},{"location":"android/#completion-options","title":"Completion Options","text":"<pre><code>val options = CompletionOptions(\n    temperature = 0.7f,\n    topP = 0.9f,\n    topK = 40,\n    maxTokens = 256,\n    stopSequences = listOf(\"\\n\\n\")\n)\n\nval result = model.complete(\"Write a haiku:\", options)\n</code></pre>"},{"location":"android/#streaming-tokens","title":"Streaming Tokens","text":"<pre><code>val result = model.complete(\n    messages = listOf(Message.user(\"Tell me a story\")),\n    callback = TokenCallback { token, tokenId -&gt;\n        print(token)\n    }\n)\n</code></pre>"},{"location":"android/#audio-transcription","title":"Audio Transcription","text":"<pre><code>val result = model.transcribe(\"/path/to/audio.wav\")\n\nval pcmData: ByteArray = ... // 16kHz mono PCM\nval result = model.transcribe(pcmData)\n</code></pre>"},{"location":"android/#embeddings","title":"Embeddings","text":"<pre><code>val embedding = model.embed(\"Hello, world!\")\nval imageEmbedding = model.imageEmbed(\"/path/to/image.jpg\")\nval audioEmbedding = model.audioEmbed(\"/path/to/audio.wav\")\n</code></pre>"},{"location":"android/#tokenization","title":"Tokenization","text":"<pre><code>val tokens = model.tokenize(\"Hello, world!\")\nval scores = model.scoreWindow(tokens, start = 0, end = tokens.size, context = 512)\n</code></pre>"},{"location":"android/#streaming-transcription","title":"Streaming Transcription","text":"<pre><code>model.createStreamTranscriber().use { stream -&gt;\n    stream.insert(audioChunk1)\n    stream.insert(audioChunk2)\n    val partial = stream.process()\n    println(\"Partial: ${partial.text}\")\n    val final = stream.finalize()\n    println(\"Final: ${final.text}\")\n}\n</code></pre>"},{"location":"android/#rag","title":"RAG","text":"<pre><code>val model = Cactus.create(\n    modelPath = \"/path/to/model\",\n    corpusDir = \"/path/to/documents\"\n)\nval result = model.complete(\"What does the documentation say about X?\")\n</code></pre>"},{"location":"android/#vector-index","title":"Vector Index","text":"<pre><code>CactusIndex.create(\"/path/to/index\", embeddingDim = 384).use { index -&gt;\n    val embeddings = arrayOf(model.embed(\"doc1\"), model.embed(\"doc2\"))\n    index.add(\n        ids = intArrayOf(1, 2),\n        documents = arrayOf(\"Document 1\", \"Document 2\"),\n        embeddings = embeddings\n    )\n    val results = index.query(model.embed(\"search query\"), topK = 5)\n    results.forEach { println(\"ID: ${it.id}, Score: ${it.score}\") }\n}\n</code></pre>"},{"location":"android/#api-reference","title":"API Reference","text":""},{"location":"android/#cactus","title":"Cactus","text":"<pre><code>object Cactus {\n    fun create(modelPath: String, corpusDir: String? = null): Cactus\n}\n\nfun complete(prompt: String, options: CompletionOptions = CompletionOptions()): CompletionResult\nfun complete(messages: List&lt;Message&gt;, options: CompletionOptions = CompletionOptions(), tools: List&lt;Map&lt;String, Any&gt;&gt;? = null, callback: TokenCallback? = null): CompletionResult\nfun transcribe(audioPath: String, prompt: String? = null, language: String? = null, translate: Boolean = false): TranscriptionResult\nfun transcribe(pcmData: ByteArray, prompt: String? = null, language: String? = null, translate: Boolean = false): TranscriptionResult\nfun embed(text: String, normalize: Boolean = true): FloatArray\nfun imageEmbed(imagePath: String): FloatArray\nfun audioEmbed(audioPath: String): FloatArray\nfun ragQuery(query: String, topK: Int = 5): String\nfun tokenize(text: String): IntArray\nfun scoreWindow(tokens: IntArray, start: Int, end: Int, context: Int): String\nfun createStreamTranscriber(): StreamTranscriber\nfun reset()\nfun stop()\nfun close()\n</code></pre>"},{"location":"android/#message","title":"Message","text":"<pre><code>data class Message(val role: String, val content: String) {\n    companion object {\n        fun system(content: String): Message\n        fun user(content: String): Message\n        fun assistant(content: String): Message\n    }\n}\n</code></pre>"},{"location":"android/#completionoptions","title":"CompletionOptions","text":"<pre><code>data class CompletionOptions(\n    val temperature: Float = 0.7f,\n    val topP: Float = 0.9f,\n    val topK: Int = 40,\n    val maxTokens: Int = 512,\n    val stopSequences: List&lt;String&gt; = emptyList(),\n    val confidenceThreshold: Float = 0f\n)\n</code></pre>"},{"location":"android/#completionresult","title":"CompletionResult","text":"<pre><code>data class CompletionResult(\n    val text: String,\n    val functionCalls: List&lt;Map&lt;String, Any&gt;&gt;?,\n    val promptTokens: Int,\n    val completionTokens: Int,\n    val timeToFirstToken: Double,\n    val totalTime: Double,\n    val prefillTokensPerSecond: Double,\n    val decodeTokensPerSecond: Double,\n    val confidence: Double,\n    val needsCloudHandoff: Boolean\n)\n</code></pre>"},{"location":"android/#transcriptionresult","title":"TranscriptionResult","text":"<pre><code>data class TranscriptionResult(\n    val text: String,\n    val segments: List&lt;Map&lt;String, Any&gt;&gt;?,\n    val totalTime: Double\n)\n</code></pre>"},{"location":"android/#tokencallback","title":"TokenCallback","text":"<pre><code>fun interface TokenCallback {\n    fun onToken(token: String, tokenId: Int)\n}\n</code></pre>"},{"location":"android/#streamtranscriber","title":"StreamTranscriber","text":"<pre><code>class StreamTranscriber : Closeable {\n    fun insert(pcmData: ByteArray)\n    fun process(language: String? = null): TranscriptionResult\n    fun finalize(): TranscriptionResult\n    fun close()\n}\n</code></pre>"},{"location":"android/#cactusindex","title":"CactusIndex","text":"<pre><code>class CactusIndex : Closeable {\n    companion object {\n        fun create(indexDir: String, embeddingDim: Int): CactusIndex\n    }\n\n    fun add(ids: IntArray, documents: Array&lt;String&gt;, embeddings: Array&lt;FloatArray&gt;, metadatas: Array&lt;String&gt;? = null)\n    fun delete(ids: IntArray)\n    fun query(embedding: FloatArray, topK: Int = 5): List&lt;IndexResult&gt;\n    fun compact()\n    fun close()\n}\n\ndata class IndexResult(val id: Int, val score: Float)\n</code></pre>"},{"location":"android/#requirements","title":"Requirements","text":"<ul> <li>Android API 24+ / arm64-v8a</li> <li>iOS 14+ / arm64 (KMP only)</li> </ul>"},{"location":"android/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 Full C API reference underlying the Kotlin bindings</li> <li>Cactus Index API \u2014 Vector database API for RAG applications</li> <li>Fine-tuning Guide \u2014 Deploy custom fine-tunes to Android</li> <li>Swift SDK \u2014 Swift alternative for Apple platforms</li> <li>Flutter SDK \u2014 Cross-platform alternative using Dart</li> </ul>"},{"location":"apple/","title":"Cactus for Swift Multiplatform","text":"<p>Run AI models on-device with a simple Swift API on iOS, macOS, and Android.</p>"},{"location":"apple/#building","title":"Building","text":"<pre><code>cactus build --apple\n</code></pre> <p>Build outputs (in <code>apple/</code>):</p> <p>see the main README.md for how to use CLI &amp; download weight</p> File Description <code>cactus-ios.xcframework/</code> iOS framework (device + simulator) <code>cactus-macos.xcframework/</code> macOS framework <code>libcactus-device.a</code> Static library for iOS device <code>libcactus-simulator.a</code> Static library for iOS simulator <p>For Android, build <code>libcactus.so</code> from the <code>android/</code> directory.</p>"},{"location":"apple/#vendored-libcurl-ios-macos","title":"Vendored libcurl (iOS + macOS)","text":"<p>To bundle libcurl from this repo instead of relying on system curl, place artifacts under:</p> <ul> <li><code>libs/curl/include/curl/*.h</code></li> <li><code>libs/curl/ios/device/libcurl.a</code></li> <li><code>libs/curl/ios/simulator/libcurl.a</code></li> <li><code>libs/curl/macos/libcurl.a</code></li> </ul> <p>Build scripts auto-detect <code>libs/curl</code>. Override with:</p> <pre><code>CACTUS_CURL_ROOT=/absolute/path/to/curl cactus build --apple\n</code></pre>"},{"location":"apple/#integration","title":"Integration","text":""},{"location":"apple/#iosmacos-xcframework-recommended","title":"iOS/macOS: XCFramework (Recommended)","text":"<ol> <li>Drag <code>cactus-ios.xcframework</code> (or <code>cactus-macos.xcframework</code>) into your Xcode project</li> <li>Ensure \"Embed &amp; Sign\" is selected in \"Frameworks, Libraries, and Embedded Content\"</li> <li>Copy <code>Cactus.swift</code> into your project</li> </ol>"},{"location":"apple/#iosmacos-static-library","title":"iOS/macOS: Static Library","text":"<ol> <li>Add <code>libcactus-device.a</code> (or <code>libcactus-simulator.a</code>) to \"Link Binary With Libraries\"</li> <li>Create a folder with <code>cactus_ffi.h</code> and <code>module.modulemap</code>, add to Build Settings:</li> <li>\"Header Search Paths\" \u2192 path to folder</li> <li>\"Import Paths\" (Swift) \u2192 path to folder</li> <li>Copy <code>Cactus.swift</code> into your project</li> </ol>"},{"location":"apple/#android-swift-sdk","title":"Android (Swift SDK)","text":"<p>Requires Swift SDK for Android.</p> <ol> <li>Copy files to your Swift project:</li> <li><code>libcactus.so</code> \u2192 your library path</li> <li><code>cactus_ffi.h</code> \u2192 your include path</li> <li><code>module.android.modulemap</code> \u2192 rename to <code>module.modulemap</code> in include path</li> <li> <p><code>Cactus.swift</code> \u2192 your sources</p> </li> <li> <p>Build with Swift SDK for Android: <pre><code>swift build --swift-sdk aarch64-unknown-linux-android28 \\\n    -Xswiftc -I/path/to/include \\\n    -Xlinker -L/path/to/lib \\\n    -Xlinker -lcactus\n</code></pre></p> </li> <li> <p>Bundle <code>libcactus.so</code> with your APK in <code>jniLibs/arm64-v8a/</code></p> </li> </ol>"},{"location":"apple/#usage","title":"Usage","text":""},{"location":"apple/#basic-completion","title":"Basic Completion","text":"<pre><code>import Foundation\n\nlet model = try Cactus(modelPath: \"/path/to/model\")\nlet result = try model.complete(\"What is the capital of France?\")\n</code></pre>"},{"location":"apple/#chat-messages","title":"Chat Messages","text":"<pre><code>let result = try model.complete(messages: [\n    .system(\"You are a helpful assistant.\"),\n    .user(\"What is 2 + 2?\")\n])\n</code></pre>"},{"location":"apple/#completion-options","title":"Completion Options","text":"<pre><code>let options = Cactus.CompletionOptions(\n    temperature: 0.7,\n    topP: 0.9,\n    topK: 40,\n    maxTokens: 256,\n    stopSequences: [\"\\n\\n\"]\n)\n\nlet result = try model.complete(\"Write a haiku:\", options: options)\n</code></pre>"},{"location":"apple/#streaming-tokens","title":"Streaming Tokens","text":"<pre><code>let result = try model.complete(\n    messages: [.user(\"Tell me a story\")],\n    onToken: { token, tokenId in\n        print(token, terminator: \"\")\n        fflush(stdout)\n    }\n)\n</code></pre>"},{"location":"apple/#asyncawait","title":"Async/Await","text":"<pre><code>let result = try await model.complete(messages: [.user(\"Hello!\")])\n\nfor try await token in model.completeStream(messages: [.user(\"Tell me a joke\")]) {\n    print(token, terminator: \"\")\n}\n</code></pre>"},{"location":"apple/#audio-transcription","title":"Audio Transcription","text":"<pre><code>// From file\nlet result = try model.transcribe(audioPath: \"/path/to/audio.wav\")\n\n// From PCM data\nlet pcmData: Data = ... // 16kHz mono PCM\nlet result = try model.transcribe(pcmData: pcmData)\n</code></pre>"},{"location":"apple/#embeddings","title":"Embeddings","text":"<pre><code>let embedding = try model.embed(text: \"Hello, world!\")\nlet imageEmbedding = try model.imageEmbed(\"/path/to/image.jpg\")\nlet audioEmbedding = try model.audioEmbed(\"/path/to/audio.wav\")\n</code></pre>"},{"location":"apple/#tokenization","title":"Tokenization","text":"<pre><code>let tokens = try model.tokenize(\"Hello, world!\")\nlet scores = try model.scoreWindow(tokens: tokens, start: 0, end: tokens.count, context: 512)\n</code></pre>"},{"location":"apple/#streaming-transcription","title":"Streaming Transcription","text":"<pre><code>let stream = try model.createStreamTranscriber()\ntry stream.insert(pcmData: audioChunk1)\ntry stream.insert(pcmData: audioChunk2)\nlet partial = try stream.process()\nprint(\"Partial: \\(partial.text)\")\nlet final = try stream.finalize()\nprint(\"Final: \\(final.text)\")\nstream.close()\n</code></pre>"},{"location":"apple/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<pre><code>let model = try Cactus(\n    modelPath: \"/path/to/model\",\n    corpusDir: \"/path/to/documents\"\n)\n\nlet result = try model.complete(\"What does the documentation say about X?\")\n</code></pre>"},{"location":"apple/#vector-index","title":"Vector Index","text":"<pre><code>let index = try CactusIndex(indexDir: \"/path/to/index\", embeddingDim: 384)\nlet embeddings = [try model.embed(text: \"doc1\"), try model.embed(text: \"doc2\")]\ntry index.add(\n    ids: [1, 2],\n    documents: [\"Document 1\", \"Document 2\"],\n    embeddings: embeddings\n)\nlet results = try index.query(embedding: try model.embed(text: \"search query\"), topK: 5)\nresults.forEach { print(\"ID: \\($0.id), Score: \\($0.score)\") }\nindex.close()\n</code></pre>"},{"location":"apple/#api-reference","title":"API Reference","text":""},{"location":"apple/#cactus","title":"Cactus","text":"<pre><code>init(modelPath: String, corpusDir: String? = nil) throws\n\nfunc complete(_ prompt: String, options: CompletionOptions = .default) throws -&gt; CompletionResult\nfunc complete(messages: [Message], options: CompletionOptions = .default, tools: [[String: Any]]? = nil, onToken: ((String, UInt32) -&gt; Void)? = nil) throws -&gt; CompletionResult\n\nfunc transcribe(audioPath: String, prompt: String? = nil, options: TranscriptionOptions = .default) throws -&gt; TranscriptionResult\nfunc transcribe(pcmData: Data, prompt: String? = nil, options: TranscriptionOptions = .default) throws -&gt; TranscriptionResult\n\nfunc embed(text: String, normalize: Bool = true) throws -&gt; [Float]\nfunc imageEmbed(_ imagePath: String) throws -&gt; [Float]\nfunc audioEmbed(_ audioPath: String) throws -&gt; [Float]\nfunc ragQuery(_ query: String, topK: Int = 5) throws -&gt; String\n\nfunc tokenize(_ text: String) throws -&gt; [UInt32]\nfunc scoreWindow(tokens: [UInt32], start: Int, end: Int, context: Int) throws -&gt; String\nfunc createStreamTranscriber() throws -&gt; StreamTranscriber\n\nfunc reset()  // Clear KV cache\nfunc stop()   // Stop generation\n</code></pre>"},{"location":"apple/#completionresult","title":"CompletionResult","text":"<pre><code>struct CompletionResult {\n    let text: String                   \n    let functionCalls: [[String: Any]]? \n    let promptTokens: Int\n    let completionTokens: Int\n    let timeToFirstToken: Double  \n    let totalTime: Double   \n    let prefillTokensPerSecond: Double\n    let decodeTokensPerSecond: Double\n    let confidence: Double  \n    let needsCloudHandoff: Bool\n}\n</code></pre>"},{"location":"apple/#message","title":"Message","text":"<pre><code>struct Message {\n    static func system(_ content: String) -&gt; Message\n    static func user(_ content: String) -&gt; Message\n    static func assistant(_ content: String) -&gt; Message\n}\n</code></pre>"},{"location":"apple/#completionoptions","title":"CompletionOptions","text":"<pre><code>struct CompletionOptions {\n    var temperature: Float = 0.7\n    var topP: Float = 0.9\n    var topK: Int = 40\n    var maxTokens: Int = 512\n    var stopSequences: [String] = []\n    var confidenceThreshold: Float = 0.0\n\n    static let `default` = CompletionOptions()\n}\n</code></pre>"},{"location":"apple/#streamtranscriber","title":"StreamTranscriber","text":"<pre><code>class StreamTranscriber {\n    func insert(pcmData: Data) throws\n    func process(language: String? = nil) throws -&gt; TranscriptionResult\n    func finalize() throws -&gt; TranscriptionResult\n    func close()\n}\n</code></pre>"},{"location":"apple/#cactusindex","title":"CactusIndex","text":"<pre><code>class CactusIndex {\n    init(indexDir: String, embeddingDim: Int) throws\n\n    func add(ids: [Int], documents: [String], embeddings: [[Float]], metadatas: [String]? = nil) throws\n    func delete(ids: [Int]) throws\n    func query(embedding: [Float], topK: Int = 5) throws -&gt; [IndexResult]\n    func compact() throws\n    func close()\n}\n\nstruct IndexResult {\n    let id: Int\n    let score: Float\n}\n</code></pre>"},{"location":"apple/#requirements","title":"Requirements","text":"<p>Apple Platforms: - iOS 14.0+ / macOS 13.0+ / tvOS 14.0+ / watchOS 7.0+ - Xcode 14.0+ - Swift 5.7+</p> <p>Android: - Swift 6.0+ with Swift SDK for Android - Android NDK 27d+ - Android API 28+ / arm64-v8a</p>"},{"location":"apple/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 Full C API reference underlying the Swift bindings</li> <li>Cactus Index API \u2014 Vector database API for RAG applications</li> <li>Fine-tuning Guide \u2014 Deploy custom fine-tunes to iOS/macOS</li> <li>Kotlin/Android SDK \u2014 Kotlin alternative for Android</li> <li>Flutter SDK \u2014 Cross-platform alternative using Dart</li> </ul>"},{"location":"blog/","title":"Cactus Blog","text":"Post Author Description Hybrid Transcription Roman Shemet Sub-150ms transcription with cloud-level accuracy using on-device/cloud hybrid inference On-Device Coding Agents Noah Cylich &amp; Henry Ndubuaku Running LFM2-24B MoE locally on Mac for coding use cases"},{"location":"blog/hybrid_transcription/","title":"Sub-150ms transcription with cloud-level accuracy: Why we built a hybrid engine","text":"<p>By Roman Shemet</p>","tags":["transcription","hybrid AI","edge AI","whisper","cloud handoff"]},{"location":"blog/hybrid_transcription/#the-voice-interface-dilemma","title":"The Voice Interface Dilemma","text":"<p>Voice is no longer a novelty; it is quickly becoming the default interface. From meeting transcriptions and dictation to voice notes and ambient computing, users expect to speak to their devices.</p> <p>But building a seamless voice experience is hard. If you've ever built a voice product, you are intimately familiar with the iron triangle of AI: Cheap, Accurate, Fast. </p> <p>Historically, you could only pick two: - Fast and Accurate? It won't be cheap. You'll need heavy cloud compute. - Fast and Cheap? It won't be accurate. You're running tiny, compromised models. - Cheap and Accurate? It won't be fast. You're waiting in API queues.</p> <p>The logic is simple. High-accuracy transcription requires large models. Large models require massive cloud resources. This introduces network latency and high infrastructure costs. </p> <p>Or, you can run small models on-device. The latency is practically zero, and compute is free. But smaller models struggle with noisy environments, accents, and complex vocabulary.</p> <p>You either pay the \"cloud tax\" in latency and cost, or you pay the \"edge tax\" in accuracy.</p>","tags":["transcription","hybrid AI","edge AI","whisper","cloud handoff"]},{"location":"blog/hybrid_transcription/#enter-hybrid-ai-the-best-of-both-worlds","title":"Enter Hybrid AI: The Best of Both Worlds","text":"<p>We didn't want to choose, so we spent the last few months building a third option at Cactus. We call it Hybrid AI.</p> <p>What if we had small, fast on-device models that were self-aware enough to know when they are struggling?</p> <p>Instead of routing 100% of user audio to an expensive cloud API, the Cactus engine processes speech locally by default. Our on-device inference provides real-time transcription with sub-150ms latency. </p> <p>However, when the engine detects messy audio (think background noise, static, or multiple speakers), it automatically hands that specific segment off to a larger model in the cloud to clean it up.</p> <p>It's like having a brilliant intern who processes 80% of the work instantly, but knows exactly when to call the senior engineer to review the tricky edge cases.</p>","tags":["transcription","hybrid AI","edge AI","whisper","cloud handoff"]},{"location":"blog/hybrid_transcription/#see-it-in-action","title":"See it in Action","text":"<p>Here is a look at the hybrid engine running locally, seamlessly managing the local-to-cloud transcription flow:</p> <p></p> <p>As a Mac user, you can pull down the CLI and test the latency and accuracy locally right now:</p> <pre><code>brew install cactus-compute/cactus/cactus\ncactus transcribe\n</code></pre>","tags":["transcription","hybrid AI","edge AI","whisper","cloud handoff"]},{"location":"blog/hybrid_transcription/#built-for-the-edge-from-the-ground-up","title":"Built for the Edge, from the Ground Up","text":"<p>Achieving this required relying on our industry-leading on-device engine. We built Cactus from the ground up exactly for this.</p> <p>Because the engine sits directly on the metal, it is optimized for edge constraints. It targets ARM-based CPUs and NPUs natively, resulting in super low RAM usage (e.g. Moonshine transcription for example, runs with sub-10MB RAM utilization). This is a critical factor for mobile and wearable devices where memory is strictly rationed by the OS.</p> <p>By keeping the footprint minimal, the engine operates silently in the background without draining battery or causing OS-level memory warnings, making it viable for continuous, ambient listening.</p>","tags":["transcription","hybrid AI","edge AI","whisper","cloud handoff"]},{"location":"blog/hybrid_transcription/#a-single-api-for-every-platform","title":"A Single API for Every Platform","text":"<p>While the underlying engine is low-level C++, we know that product teams need to move fast in their native environments. We designed the integration to be a drop-in experience regardless of your stack.</p> <p>The interface remains the same whether you are a low-level C++ engineer, or a mobile developer. You can integrate the Cactus Hybrid AI engine directly into your stack: - Mobile: React Native, Flutter, Kotlin, Swift - Systems: C++, Rust - Scripting: Python</p> <p>At the top level, the implementation is beautifully simple: initialize the model, pass the audio stream, and receive real-time text.</p> <p>A few levels deeper, developers have granular control. You can configure the engine's behavior to define a specific Word Error Rate (WER) threshold, target a strict cloud-handoff ratio, or establish hard cost limits per user.</p> <p>Check out the open-source engine and documentation on GitHub.</p>","tags":["transcription","hybrid AI","edge AI","whisper","cloud handoff"]},{"location":"blog/hybrid_transcription/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API Reference \u2014 Full transcription API docs (<code>cactus_transcribe</code>, <code>cactus_stream_transcribe_*</code>)</li> <li>Python SDK \u2014 Python bindings with transcription support</li> <li>Swift SDK \u2014 Swift API with async streaming transcription</li> <li>Kotlin/Android SDK \u2014 Kotlin API with transcription support</li> <li>Flutter SDK \u2014 Flutter bindings with streaming transcription</li> <li>LFM2 24B Review \u2014 Running large MoE models locally with Cactus</li> </ul>","tags":["transcription","hybrid AI","edge AI","whisper","cloud handoff"]},{"location":"blog/lfm2_24b_a2b/","title":"The Sweet Spot for Mac Code Use: Reviewing LFM2 24B MoE A2B with Cactus","text":"<p>By Noah Cylich and Henry Ndubuaku</p> <p></p> <p>LFM2-24B-A2B is a really great next step to see over the LFM2-8B-A1B model. The model features 24B total parameters, but only activates a sparse subset of 2B during inference. This allows it to be competitive in inference speed to 2B dense models, while delivering far greater performance.</p> <p>\"LFM2-24B-A1 excels at coding, keen to see on-device coding agents built with these.\" \u2014 Henry Ndubuaku, Cactus Co-founder &amp; CTO</p>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#architecture-breakdown","title":"Architecture Breakdown","text":"<p>Going into more depth about the model, there's really a lot to appreciate with all the architectural work LFM has accomplished, here's the breakdown:</p> <ol> <li>GQA: Grouped-query attention is the industry standard choice for efficient LLMs, their choice of a group size of 4 means that the KV cache is 4x smaller than standard, baseline attention.</li> <li>Gated Convolution: This is the signature design choice of the Liquid series of models and efficiently adds parameters and expressiveness without much compute cost.</li> <li>Efficient Vocab: The small vocab size of 65k is actually a strength for these models, as the final matmul vocab projection is the slowest static part of every model and is extremely parameter efficient. Gemma3 270m for instance dedicated 170m params just to its vocab projection since it has a vocab of 250k tokens.</li> <li>MoE: Mixture of experts is the most important choice for this model that really separates it from Liquid's prior work, it scales up parameters without sacrificing speed.</li> </ol> <p>Together with Cactus, these choices enable lightning fast inference at low energy. Ultimately, despite being 24B params, only 200mb of running memory is used, while generating 25 TPS with our m4 pro chips with 48gb of ram.</p>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#model-architecture-diagram","title":"Model Architecture Diagram","text":"<pre><code>                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                    \u2502    Linear     \u2502\n                                    \u2502Tied w/ Embed. \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                            \u2502\n                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                      \u2502   Norm    \u2502                                 \u2502 Gated Short           \u2502\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502 Convolution Block     \u2502\n                                            \u2502                                       \u2502                       \u2502\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2295\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502             \u2191         \u2502\n \u2502 SwiGLU Expert    \u2502             \u2502         \u2502             \u2502                         \u2502         \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510     \u2502\n \u2502                  \u2502             \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                         \u2502         \u2502Linear \u2502     \u2502\n \u2502        \u2191         \u2502             \u2502 \u2502     MoE Block     \u2502 \u2502                         \u2502         \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518     \u2502\n \u2502    \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510     \u2502             \u2502 \u2502         \u2191         \u2502 \u2502                         \u2502             \u2502         \u2502\n \u2502    \u2502Linear \u2502     \u2502             \u2502 \u2502         \u2295         \u2502 \u2502                         \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2297         \u2502\n \u2502    \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518     \u2502             \u2502 \u2502  \u2197    \u2197   \u2196  \u2196    \u2502 \u2502                         \u2502     \u2502       \u2191         \u2502\n \u2502        \u2502         \u2502             \u2502 \u2502 \u2297    \u2297     \u2297    \u2297 \u2502 \u2502                         \u2502 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502\n \u2502        \u2297 \u25c4\u2500\u2500\u2500\u2510   \u2502\u25c4------------\u2524 \u2502 \u254e    \u254e     \u254e    \u254e \u2502 \u2502                 \u250c------\u25ba\u2502 \u2502 Conv1D \u2502  \u2502         \u2502\n \u2502        \u2191     \u2502   \u2502             \u2502 \u2502 \u2191    \u2191     \u2191    \u2191 \u2502 \u2502                 \u254e       \u2502 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502\n \u2502        \u2502  \u250c\u2500\u2500\u2534\u2500\u2500\u2510\u2502             \u2502 \u2502 E1...E4...E9...E64\u2502 \u2502                 \u254e       \u2502     \u2502       \u2502         \u2502\n \u2502        \u2502  \u2502SiLU \u2502\u2502             \u2502 \u2502 \u2191    \u2191     \u2191    \u2191 \u2502 \u2502                 \u254e       \u2502     \u2297 \u25c4\u2500\u2500\u2500\u2510 \u2502         \u2502\n \u2502        \u2502  \u2514\u2500\u2500\u252c\u2500\u2500\u2518\u2502             \u2502 \u251c\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2524 \u2502                 \u254e       \u2502     \u2191     \u2502 \u2502         \u2502\n \u2502    \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u2502   \u2502             \u2502 \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502 \u2502                 \u254e       \u2502     B     X C         \u2502\n \u2502    \u2502Linear \u251c\u2500\u2518   \u2502             \u2502 \u2502   \u2502  Router   \u2502   \u2502 \u2502                 \u254e       \u2502     \u2191     \u2191 \u2191         \u2502\n \u2502    \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518     \u2502             \u2502 \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 \u2502                 \u254e       \u2502  \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n \u2502        \u2191         \u2502             \u2502 \u2502         \u2502         \u2502 \u2502                 \u254e       \u2502  \u2502     Linear      \u2502  \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u254e       \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                                  \u2502           \u2502           \u2502                 \u254e       \u2502           \u2191           |\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u00d7 Num of Layers \u254e       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                              \u2502                             \u254e\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510                       \u254e\n                                        \u2502   Norm    \u2502                       \u254e       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                       \u254e       \u2502 GQA Block             \u2502\n                                              \u2502                             \u254e       \u2502                       \u2502\n                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2295\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u254e       \u2502           \u2191           \u2502\n                                    \u2502         \u2502         \u2502                   \u254e       \u2502       \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510       \u2502\n                                    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                   \u2502       \u2502       \u2502Linear \u2502       \u2502\n                                    \u2502 \u2502Sequence Block \u2502 \u2502                   \u2502       \u2502       \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518       \u2502\n                                    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u251c-------------------\u2524 OR    \u2502           \u2502           \u2502\n                                    \u2502         \u2502         \u2502                   \u2502       \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502       \u2502 \u2502  Grouped Query    \u2502 \u2502\n                                              \u2502                             \u2514------\u25ba\u2502 \u2502    Attention      \u2502 \u2502\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2518 \u2502\n                                        \u2502   Norm    \u2502                               \u2502   Q       K       V   \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502   \u2191       \u2191       \u2191   \u2502\n                                              \u2502                                     \u2502 \u250c\u2500\u2534\u2500\u2500\u2510  \u250c\u2500\u2534\u2500\u2500\u2510    \u2502   \u2502\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502 \u2502Norm\u2502  \u2502Norm\u2502    \u2502   \u2502\n                                        \u2502 Embedding \u2502                               \u2502 \u2514\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u252c\u2500\u2500\u2518    \u2502   \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502   \u2191       \u2191       \u2502   \u2502\n                                              \u2502                                     \u2502 \u250c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2510 \u2502\n                                            Input                                   \u2502 \u2502      Linear       \u2502 \u2502\n                                                                                    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                                                                                    \u2502           \u2191           \u2502\n                                                                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#getting-started-with-lfm2-24b-on-cactus","title":"Getting Started with LFM2-24B on Cactus","text":"<p>Ready to run LFM2-24B locally on your Mac? Here's how to get up and running.</p>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS with Apple Silicon and 16GB+ RAM (M1 or later recommended; M4 Pro with 48GB RAM for best results)</li> <li>Python 3.10+</li> <li>CMake (<code>brew install cmake</code>)</li> <li>Git</li> </ul>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#1-clone-and-build","title":"1. Clone and Build","text":"<pre><code>git clone https://github.com/cactus-compute/cactus.git\ncd cactus\n\n# Build the Cactus engine (shared library for Python FFI)\ncactus build --python\n</code></pre>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#2-download-the-model","title":"2. Download the Model","text":"<p>Cactus handles downloading and converting HuggingFace models to its optimized binary format with INT4/INT8 quantization, all in one command:</p> <pre><code>cactus download LiquidAI/LFM2-24B-A2B\n</code></pre>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#3-chat-interactively","title":"3. Chat Interactively","text":"<p>The fastest way to start chatting with the model:</p> <pre><code>cactus run LiquidAI/LFM2-24B-A2B\n</code></pre> <p>This builds, downloads (if needed), and launches an interactive chat session.</p>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#4-use-the-python-api","title":"4. Use the Python API","text":"<p>For building your own applications and agents, use the Python FFI bindings directly:</p> <pre><code>import json\nfrom cactus import cactus_init, cactus_complete, cactus_reset, cactus_destroy\n\n# Load the model\nmodel = cactus_init(\"weights/lfm2-24b-a2b\")\n\n# Simple chat completion\nmessages = [{\"role\": \"user\", \"content\": \"Write a Python function to sort a list\"}]\nresponse = json.loads(cactus_complete(model, messages))\n\nprint(response[\"response\"])       # Generated text\nprint(f\"{response['decode_tps']:.1f} tokens/sec\")\n\n# Streaming with a callback\ndef on_token(token, token_id, user_data):\n    print(token.decode(), end=\"\", flush=True)\n\ncactus_complete(model, messages, callback=on_token)\n\n# Clean up\ncactus_reset(model)\ncactus_destroy(model)\n</code></pre>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#5-function-calling-for-agents","title":"5. Function Calling for Agents","text":"<p>Cactus supports tool use out of the box, a key building block for on-device coding agents:</p> <pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"run_code\",\n            \"description\": \"Execute Python code and return the output\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"code\": {\"type\": \"string\", \"description\": \"Python code to execute\"}\n                },\n                \"required\": [\"code\"]\n            }\n        }\n    }\n]\n\nmessages = [{\"role\": \"user\", \"content\": \"Calculate the factorial of 10\"}]\nresponse = json.loads(cactus_complete(model, messages, tools=tools))\n\nif response[\"function_calls\"]:\n    print(response[\"function_calls\"])  # Model's tool invocation\n</code></pre>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#cloud-handoff","title":"Cloud Handoff","text":"<p>Cactus measures model confidence during generation. When the model isn't confident enough for a query, the response signals <code>cloud_handoff: true</code>, letting your agent route complex requests to a cloud API while keeping simple ones fast and local:</p> <pre><code>response = json.loads(cactus_complete(model, messages, confidence_threshold=0.7))\n\nif response[\"cloud_handoff\"]:\n    # Route to cloud API for this query\n    pass\nelse:\n    print(response[\"response\"])\n</code></pre> <p>This hybrid local-cloud pattern is what makes on-device coding agents practical: fast local inference for the majority of tasks, with automatic escalation when needed.</p>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#conclusion","title":"Conclusion","text":"<p>LFM2-24B-A2B represents a compelling sweet spot for on-device coding. The MoE architecture activates just 2B of its 24B parameters per token, delivering quality that punches well above its compute class while keeping inference fast and memory-lean at ~200MB of running RAM. Paired with Cactus's INT4 quantization, SIMD-optimized kernels, and built-in function calling, this is a model you can actually build local coding agents on top of today.</p> <p>The pieces are coming together: models that are smart enough to be useful, efficient enough to run on a laptop, and runtimes that make it all accessible through a few lines of Python. Whether you're building a code assistant that works offline, a privacy-first dev tool, or just experimenting with what's possible without a cloud API, LFM2-24B with Cactus is a great place to start.</p> <p>Give it a try, build something, and let us know what you think.</p>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"blog/lfm2_24b_a2b/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API Reference \u2014 Full C API docs for completion, tool calling, and cloud handoff</li> <li>Python SDK \u2014 Python bindings used in the examples above</li> <li>Fine-tuning Guide \u2014 Deploy your own LoRA fine-tunes to mobile</li> <li>Hybrid Transcription \u2014 On-device/cloud hybrid speech transcription with Cactus</li> <li>Runtime Compatibility \u2014 Weight versioning across Cactus releases</li> </ul>","tags":["LFM2","MoE","coding agents","Apple Silicon","Python","function calling"]},{"location":"docs/cactus_engine/","title":"Cactus Engine FFI Documentation","text":"<p>The Cactus Engine provides a clean C FFI (Foreign Function Interface) for integrating the LLM inference engine into various applications. This documentation covers all available functions, their parameters, and usage examples.</p>"},{"location":"docs/cactus_engine/#getting-started","title":"Getting Started","text":"<p>Before using the Cactus Engine, you need to download model weights:</p> <pre><code>./setup\ncactus download LiquidAI/LFM2-1.2B\ncactus download LiquidAI/LFM2-VL-450M\ncactus download openai/whisper-small\ncactus download UsefulSensors/moonshine-base --precision FP16\n\n# Optional: set your Cactus Cloud API key for automatic cloud fallback\ncactus auth\n</code></pre> <p>Weights are saved to the <code>weights/</code> directory and can be loaded using <code>cactus_init()</code>. Moonshine requires FP16 precision when downloading and running.</p>"},{"location":"docs/cactus_engine/#types","title":"Types","text":""},{"location":"docs/cactus_engine/#cactus_model_t","title":"<code>cactus_model_t</code>","text":"<p>An opaque pointer type representing a loaded model instance. This handle is used throughout the API to reference a specific model.</p> <pre><code>typedef void* cactus_model_t;\n</code></pre>"},{"location":"docs/cactus_engine/#cactus_token_callback","title":"<code>cactus_token_callback</code>","text":"<p>Callback function type for streaming token generation. Called for each generated token during completion.</p> <pre><code>typedef void (*cactus_token_callback)(\n    const char* token,      // The generated token text\n    uint32_t token_id,      // The token's ID in the vocabulary\n    void* user_data         // User-provided context data\n);\n</code></pre>"},{"location":"docs/cactus_engine/#core-functions","title":"Core Functions","text":""},{"location":"docs/cactus_engine/#cactus_init","title":"<code>cactus_init</code>","text":"<p>Initializes a model from disk and prepares it for inference.</p> <pre><code>cactus_model_t cactus_init(\n    const char* model_path,   // Path to the model directory\n    const char* corpus_dir    // Optional path to corpus directory for RAG (can be NULL)\n);\n</code></pre> <p>Returns: Model handle on success, NULL on failure</p> <p>Example: <pre><code>cactus_model_t model = cactus_init(\"../../weights/qwen3-600m\", NULL);\nif (!model) {\n    fprintf(stderr, \"Failed to initialize model\\n\");\n    return -1;\n}\n\n// with RAG corpus\ncactus_model_t rag_model = cactus_init(\"../../weights/lfm2-rag\", \"./documents\");\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_complete","title":"<code>cactus_complete</code>","text":"<p>Performs text completion with optional streaming and tool support.</p> <pre><code>int cactus_complete(\n    cactus_model_t model,           // Model handle\n    const char* messages_json,      // JSON array of messages\n    char* response_buffer,          // Buffer for response JSON\n    size_t buffer_size,             // Size of response buffer\n    const char* options_json,       // Optional generation options (can be NULL)\n    const char* tools_json,         // Optional tools definition (can be NULL)\n    cactus_token_callback callback, // Optional streaming callback (can be NULL)\n    void* user_data                 // User data for callback (can be NULL)\n);\n</code></pre> <p>Returns: Number of bytes written to response_buffer on success, negative value on error</p> <p>Message Format: <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is your name?\"}\n]\n</code></pre></p> <p>Messages with Images (for VLM models): <pre><code>[\n    {\"role\": \"user\", \"content\": \"Describe this image\", \"images\": [\"/path/to/image.jpg\"]}\n]\n</code></pre></p> <p>Options Format: <pre><code>{\n    \"max_tokens\": 256,\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"stop_sequences\": [\"&lt;|im_end|&gt;\", \"&lt;end_of_turn&gt;\"],\n    \"force_tools\": false,\n    \"tool_rag_top_k\": 2,\n    \"confidence_threshold\": 0.7\n}\n</code></pre></p> Option Type Default Description <code>max_tokens</code> int 100 Maximum tokens to generate <code>temperature</code> float 0.0 Sampling temperature <code>top_p</code> float 0.0 Top-p (nucleus) sampling <code>top_k</code> int 0 Top-k sampling <code>stop_sequences</code> array [] Stop generation on these strings <code>force_tools</code> bool false Constrain output to tool call format <code>tool_rag_top_k</code> int 2 Select top-k relevant tools via Tool RAG (0 = disabled, use all tools) <code>confidence_threshold</code> float 0.7 Minimum confidence for local generation; triggers cloud_handoff when below <p>Response Format (all fields always present): <pre><code>{\n    \"success\": true,\n    \"error\": null,\n    \"cloud_handoff\": false,\n    \"response\": \"I am an AI assistant.\",\n    \"function_calls\": [],\n    \"confidence\": 0.85,\n    \"time_to_first_token_ms\": 150.5,\n    \"total_time_ms\": 1250.3,\n    \"prefill_tps\": 166.1,\n    \"decode_tps\": 45.2,\n    \"ram_usage_mb\": 245.67,\n    \"prefill_tokens\": 25,\n    \"decode_tokens\": 8,\n    \"total_tokens\": 33\n}\n</code></pre></p> <p>Cloud Handoff Response (when model detects low confidence): <pre><code>{\n    \"success\": false,\n    \"error\": null,\n    \"cloud_handoff\": true,\n    \"response\": null,\n    \"function_calls\": [],\n    \"confidence\": 0.18,\n    \"time_to_first_token_ms\": 45.2,\n    \"total_time_ms\": 45.2,\n    \"prefill_tps\": 619.5,\n    \"decode_tps\": 0.0,\n    \"ram_usage_mb\": 245.67,\n    \"prefill_tokens\": 28,\n    \"decode_tokens\": 0,\n    \"total_tokens\": 28\n}\n</code></pre></p> <p>When <code>cloud_handoff</code> is true, the model's confidence dropped below <code>confidence_threshold</code> (default: 0.7). The application should defer to a cloud-based model for better results.</p> <p>Error Response: <pre><code>{\n    \"success\": false,\n    \"error\": \"Error message here\",\n    \"cloud_handoff\": false,\n    \"response\": null,\n    \"function_calls\": [],\n    \"confidence\": 0.0,\n    \"time_to_first_token_ms\": 0.0,\n    \"total_time_ms\": 0.0,\n    \"prefill_tps\": 0.0,\n    \"decode_tps\": 0.0,\n    \"ram_usage_mb\": 0.0,\n    \"prefill_tokens\": 0,\n    \"decode_tokens\": 0,\n    \"total_tokens\": 0\n}\n</code></pre></p> <p>Response with Function Call: <pre><code>{\n    \"success\": true,\n    \"error\": null,\n    \"cloud_handoff\": false,\n    \"response\": \"\",\n    \"function_calls\": [\n        {\n            \"name\": \"get_weather\",\n            \"arguments\": \"{\\\"location\\\": \\\"San Francisco, CA, USA\\\"}\"\n        }\n    ],\n    \"confidence\": 0.92,\n    \"time_to_first_token_ms\": 120.0,\n    \"total_time_ms\": 450.5,\n    \"prefill_tps\": 375.0,\n    \"decode_tps\": 38.5,\n    \"ram_usage_mb\": 245.67,\n    \"prefill_tokens\": 45,\n    \"decode_tokens\": 15,\n    \"total_tokens\": 60\n}\n</code></pre></p> <p>Example with Streaming: <pre><code>void streaming_callback(const char* token, uint32_t token_id, void* user_data) {\n    printf(\"%s\", token);\n    fflush(stdout);\n}\n\nconst char* messages = \"[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Tell me a story\\\"}]\";\n\nchar response[8192];\nint result = cactus_complete(model, messages, response, sizeof(response),\n                             NULL, NULL, streaming_callback, NULL);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_tokenize","title":"<code>cactus_tokenize</code>","text":"<p>Tokenizes text into token IDs using the model's tokenizer.</p> <pre><code>int cactus_tokenize(\n    cactus_model_t model,        // Model handle\n    const char* text,            // Text to tokenize\n    uint32_t* token_buffer,      // Buffer for token IDs\n    size_t token_buffer_len,     // Maximum number of tokens buffer can hold\n    size_t* out_token_len        // Output: actual number of tokens\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>const char* text = \"Hello, world!\";\nuint32_t tokens[256];\nsize_t num_tokens = 0;\n\nint result = cactus_tokenize(model, text, tokens, 256, &amp;num_tokens);\nif (result == 0) {\n    printf(\"Tokenized into %zu tokens: \", num_tokens);\n    for (size_t i = 0; i &lt; num_tokens; i++) {\n        printf(\"%u \", tokens[i]);\n    }\n    printf(\"\\n\");\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_score_window","title":"<code>cactus_score_window</code>","text":"<p>Scores a window of tokens for perplexity calculation or token probability analysis.</p> <pre><code>int cactus_score_window(\n    cactus_model_t model,        // Model handle\n    const uint32_t* tokens,      // Array of token IDs\n    size_t token_len,            // Total number of tokens\n    size_t start,                // Start index of window to score\n    size_t end,                  // End index of window to score\n    size_t context,              // Context window size\n    char* response_buffer,       // Buffer for response JSON\n    size_t buffer_size           // Size of response buffer\n);\n</code></pre> <p>Returns: Number of bytes written to response_buffer on success, negative value on error</p> <p>Example: <pre><code>uint32_t tokens[256];\nsize_t num_tokens;\ncactus_tokenize(model, \"The quick brown fox\", tokens, 256, &amp;num_tokens);\n\nchar response[4096];\nint result = cactus_score_window(model, tokens, num_tokens, 0, num_tokens, 512,\n                                  response, sizeof(response));\nif (result &gt; 0) {\n    printf(\"Scores: %s\\n\", response);\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_transcribe","title":"<code>cactus_transcribe</code>","text":"<p>Transcribes audio to text using a Whisper model. Supports both file-based and buffer-based audio input.</p> <pre><code>int cactus_transcribe(\n    cactus_model_t model,           // Model handle (must be Whisper model)\n    const char* audio_file_path,    // Path to audio file (WAV, MP3, etc.) - can be NULL if using pcm_buffer\n    const char* prompt,             // Optional prompt to guide transcription (can be NULL)\n    char* response_buffer,          // Buffer for response JSON\n    size_t buffer_size,             // Size of response buffer\n    const char* options_json,       // Optional transcription options (can be NULL)\n    cactus_token_callback callback, // Optional streaming callback (can be NULL)\n    void* user_data,                // User data for callback (can be NULL)\n    const uint8_t* pcm_buffer,      // Optional raw PCM audio buffer (can be NULL if using file)\n    size_t pcm_buffer_size          // Size of PCM buffer in bytes\n);\n</code></pre> <p>Returns: Number of bytes written to response_buffer on success, negative value on error</p> <p>Example (file-based): <pre><code>cactus_model_t whisper = cactus_init(\"../../weights/whisper-small\", NULL);\n\nchar response[16384];\nint result = cactus_transcribe(whisper, \"audio.wav\", NULL,\n                                response, sizeof(response), NULL, NULL, NULL,\n                                NULL, 0);\nif (result &gt; 0) {\n    printf(\"Transcription: %s\\n\", response);\n}\n</code></pre></p> <p>Example (buffer-based): <pre><code>uint8_t* pcm_data = load_audio_buffer(\"audio.wav\", &amp;pcm_size); // 16kHz, mono, 16-bit\n\nchar response[16384];\nint result = cactus_transcribe(whisper, NULL, NULL,\n                                response, sizeof(response), NULL, NULL, NULL,\n                                pcm_data, pcm_size);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_stream_transcribe_t","title":"<code>cactus_stream_transcribe_t</code>","text":"<p>An opaque pointer type representing a streaming transcription session. Used for real-time audio transcription with incremental confirmation.</p> <pre><code>typedef void* cactus_stream_transcribe_t;\n</code></pre>"},{"location":"docs/cactus_engine/#cactus_stream_transcribe_start","title":"<code>cactus_stream_transcribe_start</code>","text":"<p>Initializes a new streaming transcription session with optional configuration.</p> <pre><code>cactus_stream_transcribe_t cactus_stream_transcribe_start(\n    cactus_model_t model,        // Model handle\n    const char* options_json     // Optional configuration (can be NULL)\n);\n</code></pre> <p>Returns: Stream handle on success, NULL on failure</p> <p>Options Format: <pre><code>{\n    \"confirmation_threshold\": 0.99,\n    \"min_chunk_size\": 32000,\n    \"language\": \"en\"\n}\n</code></pre></p> <ul> <li><code>confirmation_threshold</code>: Threshold (0.0-1.0) for confirming transcription segments. Higher values require more stability. Default: 0.99</li> <li><code>min_chunk_size</code>: Minimum audio samples to perform transcription processing step. Default: 32000</li> <li><code>language</code>: ISO 639-1 language code (e.g., \"en\", \"es\", \"fr\", \"de\"). Default: \"en\". Ignored for non-Whisper models.</li> </ul> <p>Example: <pre><code>cactus_model_t whisper = cactus_init(\"../../weights/whisper-small\", NULL);\n\ncactus_stream_transcribe_t stream = cactus_stream_transcribe_start(whisper, \"{\\\"confirmation_threshold\\\": 1.0}\");\nif (!stream) {\n    fprintf(stderr, \"Failed to start stream: %s\\n\", cactus_get_last_error());\n    return -1;\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_stream_transcribe_process","title":"<code>cactus_stream_transcribe_process</code>","text":"<p>Processes audio chunk and returns confirmed and pending transcription results.</p> <pre><code>int cactus_stream_transcribe_process(\n    cactus_stream_transcribe_t stream,  // Stream handle\n    const uint8_t* pcm_buffer,          // Raw PCM audio (16-bit, 16kHz, mono)\n    size_t pcm_buffer_size,             // Size of PCM buffer in bytes\n    char* response_buffer,              // Buffer for response JSON\n    size_t buffer_size                  // Size of response buffer\n);\n</code></pre> <p>Returns: Number of bytes written to response_buffer on success, negative value on error</p> <p>Response Format: <pre><code>{\n    \"success\": true,\n    \"error\": null,\n    \"cloud_handoff\": false,\n    \"confirmed\": \"text confirmed from previous call\",\n    \"pending\": \"current transcription result\",\n    \"function_calls\": [],\n    \"confidence\": 0.95,\n    \"time_to_first_token_ms\": 150.5,\n    \"total_time_ms\": 450.2,\n    \"prefill_tps\": 100.0,\n    \"decode_tps\": 50.0,\n    \"ram_usage_mb\": 512.5,\n    \"prefill_tokens\": 100,\n    \"decode_tokens\": 50,\n    \"total_tokens\": 150\n}\n</code></pre></p> <ul> <li><code>confirmed</code>: Confirmed transcription chunk</li> <li><code>pending</code>: Current transcription result (may be confirmed in next call if stable)</li> <li><code>error</code>: Error message if any, null otherwise</li> <li><code>cloud_handoff</code>: Whether cloud handoff is needed</li> <li><code>function_calls</code>: Array of function calls if any</li> <li><code>confidence</code>, timing, and token metrics: Model performance statistics</li> </ul> <p>Example: <pre><code>uint8_t audio_chunk[32000]; // 1 second at 16kHz, 16-bit\nchar response[32768];\n\nint result = cactus_stream_transcribe_process(stream, audio_chunk, sizeof(audio_chunk), response, sizeof(response));\nif (result &gt; 0) {\n    printf(\"Response: %s\\n\", response);\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_stream_transcribe_stop","title":"<code>cactus_stream_transcribe_stop</code>","text":"<p>Stops the streaming session and returns any remaining confirmed transcription. Releases all resources.</p> <pre><code>int cactus_stream_transcribe_stop(\n    cactus_stream_transcribe_t stream,  // Stream handle\n    char* response_buffer,              // Buffer for response JSON (can be NULL)\n    size_t buffer_size                  // Size of response buffer (can be 0)\n);\n</code></pre> <p>Returns: Number of bytes written on success, 0 if no response buffer provided, negative value on error</p> <p>Response Format: <pre><code>{\n    \"success\": true,\n    \"confirmed\": \"Final confirmed transcription chunk\"\n}\n</code></pre></p> <p>Example: <pre><code>char final_response[32768];\nint result = cactus_stream_transcribe_stop(stream, final_response, sizeof(final_response));\nif (result &gt; 0) {\n    printf(\"Final: %s\\n\", final_response);\n}\n\n// Or simply cleanup resources without response\ncactus_stream_transcribe_stop(stream, NULL, 0);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_vad","title":"<code>cactus_vad</code>","text":"<p>Detects speech segments in audio using Voice Activity Detection. Supports both file-based and buffer-based audio input.</p> <pre><code>int cactus_vad(\n    cactus_model_t model,           // Model handle (must be VAD model)\n    const char* audio_file_path,    // Path to audio file - can be NULL if using pcm_buffer\n    char* response_buffer,          // Buffer for response JSON\n    size_t buffer_size,             // Size of response buffer\n    const char* options_json,       // Optional VAD options (can be NULL)\n    const uint8_t* pcm_buffer,      // Optional raw PCM audio buffer (can be NULL if using file)\n    size_t pcm_buffer_size          // Size of PCM buffer in bytes\n);\n</code></pre> <p>Returns: Number of bytes written to response_buffer on success, negative value on error</p> <p>Response Format: <pre><code>{\n  \"success\": true,\n  \"error\": null,\n  \"segments\": [\n    {\"start\": 0, \"end\": 16000},\n    {\"start\": 32000, \"end\": 48000}\n  ],\n  \"total_time_ms\": 12.34,\n  \"ram_usage_mb\": 45.67\n}\n</code></pre></p> <p>Example: <pre><code>cactus_model_t vad = cactus_init(\"../../weights/silero-vad\", NULL);\n\nchar response[4096];\nint result = cactus_vad(vad, \"audio.wav\", response, sizeof(response), NULL, NULL, 0);\n\nif (result &gt; 0) {\n    printf(\"Response: %s\\n\", response);\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_embed","title":"<code>cactus_embed</code>","text":"<p>Generates text embeddings for semantic search, similarity, and RAG applications.</p> <pre><code>int cactus_embed(\n    cactus_model_t model,        // Model handle\n    const char* text,            // Text to embed\n    float* embeddings_buffer,    // Buffer for embedding vector\n    size_t buffer_size,          // Buffer size in bytes\n    size_t* embedding_dim,       // Output: actual embedding dimensions\n    bool normalize               // Whether to L2-normalize the output vector\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>const char* text = \"The quick brown fox jumps over the lazy dog\";\nfloat embeddings[2048];\nsize_t actual_dim = 0;\n\nint result = cactus_embed(model, text, embeddings, sizeof(embeddings), &amp;actual_dim, true);\nif (result == 0) {\n    printf(\"Generated %zu-dimensional embedding\\n\", actual_dim);\n}\n</code></pre></p> <p>Note: Set <code>normalize</code> to <code>true</code> for cosine similarity comparisons (recommended for most use cases).</p>"},{"location":"docs/cactus_engine/#cactus_image_embed","title":"<code>cactus_image_embed</code>","text":"<p>Generates embeddings for images, useful for multimodal retrieval tasks.</p> <pre><code>int cactus_image_embed(\n    cactus_model_t model,        // Model handle (must support vision)\n    const char* image_path,      // Path to image file\n    float* embeddings_buffer,    // Buffer for embedding vector\n    size_t buffer_size,          // Buffer size in bytes\n    size_t* embedding_dim        // Output: actual embedding dimensions\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>float image_embeddings[1024];\nsize_t dim = 0;\n\nint result = cactus_image_embed(model, \"photo.jpg\", image_embeddings,\n                                 sizeof(image_embeddings), &amp;dim);\nif (result == 0) {\n    printf(\"Image embedding dimension: %zu\\n\", dim);\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_audio_embed","title":"<code>cactus_audio_embed</code>","text":"<p>Generates embeddings for audio files, useful for audio retrieval and classification.</p> <pre><code>int cactus_audio_embed(\n    cactus_model_t model,        // Model handle (must support audio)\n    const char* audio_path,      // Path to audio file\n    float* embeddings_buffer,    // Buffer for embedding vector\n    size_t buffer_size,          // Buffer size in bytes\n    size_t* embedding_dim        // Output: actual embedding dimensions\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>float audio_embeddings[768];\nsize_t dim = 0;\n\nint result = cactus_audio_embed(model, \"speech.wav\", audio_embeddings,\n                                 sizeof(audio_embeddings), &amp;dim);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_stop","title":"<code>cactus_stop</code>","text":"<p>Stops ongoing generation. Useful for implementing early stopping based on custom logic.</p> <pre><code>void cactus_stop(cactus_model_t model);\n</code></pre> <p>Example with Controlled Generation: <pre><code>struct ControlData {\n    cactus_model_t model;\n    int token_count;\n    int max_tokens;\n};\n\nvoid control_callback(const char* token, uint32_t token_id, void* user_data) {\n    struct ControlData* data = (struct ControlData*)user_data;\n    printf(\"%s\", token);\n    data-&gt;token_count++;\n\n    // Stop after reaching limit\n    if (data-&gt;token_count &gt;= data-&gt;max_tokens) {\n        cactus_stop(data-&gt;model);\n    }\n}\n\nstruct ControlData control = {model, 0, 50};\ncactus_complete(model, messages, response, sizeof(response),\n                NULL, NULL, control_callback, &amp;control);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_reset","title":"<code>cactus_reset</code>","text":"<p>Resets the model's internal state, clearing KV cache and any cached context.</p> <pre><code>void cactus_reset(cactus_model_t model);\n</code></pre> <p>Use Cases: - Starting a new conversation - Clearing context between unrelated requests - Recovering from errors - Freeing memory after long conversations</p>"},{"location":"docs/cactus_engine/#cactus_rag_query","title":"<code>cactus_rag_query</code>","text":"<p>Queries the RAG corpus and returns relevant text chunks. Requires model to be initialized with a corpus directory.</p> <pre><code>int cactus_rag_query(\n    cactus_model_t model,        // Model handle (must have corpus_dir set)\n    const char* query,           // Query text\n    char* response_buffer,       // Buffer for response JSON\n    size_t buffer_size,          // Size of response buffer\n    size_t top_k                 // Number of chunks to retrieve\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Response Format: <pre><code>[\n    {\"text\": \"Relevant chunk 1...\", \"score\": 0.85},\n    {\"text\": \"Relevant chunk 2...\", \"score\": 0.72}\n]\n</code></pre></p> <p>Example: <pre><code>// Initialize model with corpus\ncactus_model_t model = cactus_init(\"path/to/model\", \"./documents\");\n\n// Query for relevant chunks\nchar response[65536];\nint result = cactus_rag_query(model, \"What is machine learning?\",\n                               response, sizeof(response), 5);\nif (result == 0) {\n    printf(\"Retrieved chunks: %s\\n\", response);\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_destroy","title":"<code>cactus_destroy</code>","text":"<p>Releases all resources associated with the model.</p> <pre><code>void cactus_destroy(cactus_model_t model);\n</code></pre> <p>Important: Always call this when done with a model to prevent memory leaks.</p>"},{"location":"docs/cactus_engine/#utility-functions","title":"Utility Functions","text":""},{"location":"docs/cactus_engine/#cactus_get_last_error","title":"<code>cactus_get_last_error</code>","text":"<p>Returns the last error message from the Cactus engine.</p> <pre><code>const char* cactus_get_last_error(void);\n</code></pre> <p>Returns: Error message string, or NULL if no error</p> <p>Example: <pre><code>cactus_model_t model = cactus_init(\"invalid/path\", NULL);\nif (!model) {\n    const char* error = cactus_get_last_error();\n    fprintf(stderr, \"Error: %s\\n\", error);\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#vector-index-apis","title":"Vector Index APIs","text":"<p>The vector index APIs provide persistent storage and retrieval of embeddings for RAG (Retrieval-Augmented Generation) applications.</p>"},{"location":"docs/cactus_engine/#cactus_index_t","title":"<code>cactus_index_t</code>","text":"<p>An opaque pointer type representing a vector index instance.</p> <pre><code>typedef void* cactus_index_t;\n</code></pre>"},{"location":"docs/cactus_engine/#cactus_index_init","title":"<code>cactus_index_init</code>","text":"<p>Initializes or opens a vector index from disk.</p> <pre><code>cactus_index_t cactus_index_init(\n    const char* index_dir,       // Path to index directory\n    size_t embedding_dim         // Dimension of embeddings to store\n);\n</code></pre> <p>Returns: Index handle on success, NULL on failure</p> <p>Example: <pre><code>cactus_index_t index = cactus_index_init(\"./my_index\", 768);\nif (!index) {\n    fprintf(stderr, \"Failed to initialize index\\n\");\n    return -1;\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_index_add","title":"<code>cactus_index_add</code>","text":"<p>Adds documents with their embeddings to the index.</p> <pre><code>int cactus_index_add(\n    cactus_index_t index,        // Index handle\n    const int* ids,              // Array of document IDs\n    const char** documents,      // Array of document texts\n    const char** metadatas,      // Array of metadata JSON strings (can be NULL)\n    const float** embeddings,    // Array of embedding vectors\n    size_t count,                // Number of documents to add\n    size_t embedding_dim         // Dimension of each embedding\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>int ids[] = {1, 2, 3};\nconst char* docs[] = {\"Hello world\", \"Foo bar\", \"Test document\"};\nconst char* metas[] = {\"{\\\"source\\\":\\\"a\\\"}\", \"{\\\"source\\\":\\\"b\\\"}\", NULL};\n\nfloat emb1[768], emb2[768], emb3[768];\nconst float* embeddings[] = {emb1, emb2, emb3};\n\nint result = cactus_index_add(index, ids, docs, metas, embeddings, 3, 768);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_index_delete","title":"<code>cactus_index_delete</code>","text":"<p>Deletes documents from the index by ID.</p> <pre><code>int cactus_index_delete(\n    cactus_index_t index,        // Index handle\n    const int* ids,              // Array of document IDs to delete\n    size_t ids_count             // Number of IDs\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>int ids_to_delete[] = {1, 3};\ncactus_index_delete(index, ids_to_delete, 2);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_index_get","title":"<code>cactus_index_get</code>","text":"<p>Retrieves documents by their IDs.</p> <pre><code>int cactus_index_get(\n    cactus_index_t index,        // Index handle\n    const int* ids,              // Array of document IDs to retrieve\n    size_t ids_count,            // Number of IDs\n    char** document_buffers,     // Output: document text buffers\n    size_t* document_buffer_sizes,  // Sizes of document buffers\n    char** metadata_buffers,     // Output: metadata JSON buffers\n    size_t* metadata_buffer_sizes,  // Sizes of metadata buffers\n    float** embedding_buffers,   // Output: embedding buffers\n    size_t* embedding_buffer_sizes  // Sizes of embedding buffers (in bytes)\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p>"},{"location":"docs/cactus_engine/#cactus_index_query","title":"<code>cactus_index_query</code>","text":"<p>Queries the index for similar documents using embedding vectors.</p> <pre><code>int cactus_index_query(\n    cactus_index_t index,        // Index handle\n    const float** embeddings,    // Array of query embeddings\n    size_t embeddings_count,     // Number of query embeddings\n    size_t embedding_dim,        // Dimension of each embedding\n    const char* options_json,    // Query options (e.g., {\"k\": 10})\n    int** id_buffers,            // Output: arrays of result IDs\n    size_t* id_buffer_sizes,     // Sizes of ID buffers\n    float** score_buffers,       // Output: arrays of similarity scores\n    size_t* score_buffer_sizes   // Sizes of score buffers\n);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>float query_emb[768];\ncactus_embed(model, \"search query\", query_emb, sizeof(query_emb), &amp;dim, true);\n\nconst float* queries[] = {query_emb};\nint result_ids[10];\nfloat result_scores[10];\nint* id_bufs[] = {result_ids};\nfloat* score_bufs[] = {result_scores};\nsize_t id_sizes[] = {10};\nsize_t score_sizes[] = {10};\n\ncactus_index_query(index, queries, 1, 768, \"{\\\"k\\\": 10}\",\n                   id_bufs, id_sizes, score_bufs, score_sizes);\n\nfor (int i = 0; i &lt; 10; i++) {\n    printf(\"ID: %d, Score: %.4f\\n\", result_ids[i], result_scores[i]);\n}\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_index_compact","title":"<code>cactus_index_compact</code>","text":"<p>Compacts the index to optimize storage and query performance.</p> <pre><code>int cactus_index_compact(cactus_index_t index);\n</code></pre> <p>Returns: 0 on success, negative value on error</p> <p>Example: <pre><code>cactus_index_compact(index);\n</code></pre></p>"},{"location":"docs/cactus_engine/#cactus_index_destroy","title":"<code>cactus_index_destroy</code>","text":"<p>Releases all resources associated with the index.</p> <pre><code>void cactus_index_destroy(cactus_index_t index);\n</code></pre> <p>Important: Always call this when done with an index to ensure data is persisted.</p>"},{"location":"docs/cactus_engine/#complete-rag-example","title":"Complete RAG Example","text":"<pre><code>#include \"cactus_ffi.h\"\n\nint main() {\n    cactus_model_t embed_model = cactus_init(\"path/to/embed-model\", NULL);\n    cactus_index_t index = cactus_index_init(\"./rag_index\", 768);\n\n    const char* docs[] = {\n        \"The capital of France is Paris.\",\n        \"Python is a programming language.\",\n        \"The Earth orbits the Sun.\"\n    };\n    int ids[] = {1, 2, 3};\n    float emb1[768], emb2[768], emb3[768];\n    size_t dim;\n\n    cactus_embed(embed_model, docs[0], emb1, sizeof(emb1), &amp;dim, true);\n    cactus_embed(embed_model, docs[1], emb2, sizeof(emb2), &amp;dim, true);\n    cactus_embed(embed_model, docs[2], emb3, sizeof(emb3), &amp;dim, true);\n\n    const float* embeddings[] = {emb1, emb2, emb3};\n    cactus_index_add(index, ids, docs, NULL, embeddings, 3, 768);\n\n    float query_emb[768];\n    cactus_embed(embed_model, \"What is the capital of France?\", query_emb, sizeof(query_emb), &amp;dim, true);\n\n    const float* queries[] = {query_emb};\n    int result_ids[3];\n    float result_scores[3];\n    int* id_bufs[] = {result_ids};\n    float* score_bufs[] = {result_scores};\n    size_t id_sizes[] = {3};\n    size_t score_sizes[] = {3};\n\n    cactus_index_query(index, queries, 1, 768, \"{\\\"k\\\": 3}\",\n                       id_bufs, id_sizes, score_bufs, score_sizes);\n\n    printf(\"Top result ID: %d (score: %.4f)\\n\", result_ids[0], result_scores[0]);\n\n    cactus_index_destroy(index);\n    cactus_destroy(embed_model);\n    return 0;\n}\n</code></pre>"},{"location":"docs/cactus_engine/#complete-examples","title":"Complete Examples","text":""},{"location":"docs/cactus_engine/#basic-conversation","title":"Basic Conversation","text":"<pre><code>#include \"cactus_ffi.h\"\n#include &lt;stdio.h&gt;\n\nint main() {\n    cactus_model_t model = cactus_init(\"path/to/model\", NULL);\n    if (!model) return -1;\n\n    const char* messages =\n        \"[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\"\n        \" {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"},\"\n        \" {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Hello! How can I help you today?\\\"},\"\n        \" {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What's 2+2?\\\"}]\";\n\n    char response[4096];\n    int result = cactus_complete(model, messages, response,\n                                 sizeof(response), NULL, NULL, NULL, NULL);\n    if (result &gt; 0) {\n        printf(\"Response: %s\\n\", response);\n    }\n\n    cactus_destroy(model);\n    return 0;\n}\n</code></pre>"},{"location":"docs/cactus_engine/#vision-language-model-vlm","title":"Vision-Language Model (VLM)","text":"<pre><code>#include \"cactus_ffi.h\"\n\nint main() {\n    cactus_model_t vlm = cactus_init(\"path/to/lfm2-vlm\", NULL);\n    if (!vlm) return -1;\n\n    const char* messages =\n        \"[{\\\"role\\\": \\\"user\\\",\"\n        \"  \\\"content\\\": \\\"What do you see in this image?\\\",\"\n        \"  \\\"images\\\": [\\\"/path/to/photo.jpg\\\"]}]\";\n\n    char response[8192];\n    int result = cactus_complete(vlm, messages, response, sizeof(response),\n                                 NULL, NULL, NULL, NULL);\n    if (result &gt; 0) {\n        printf(\"%s\\n\", response);\n    }\n\n    cactus_destroy(vlm);\n    return 0;\n}\n</code></pre>"},{"location":"docs/cactus_engine/#tool-calling","title":"Tool Calling","text":"<pre><code>const char* tools =\n    \"[{\\\"function\\\": {\"\n    \"    \\\"name\\\": \\\"get_weather\\\",\"\n    \"    \\\"description\\\": \\\"Get weather for a location\\\",\"\n    \"    \\\"parameters\\\": {\"\n    \"        \\\"type\\\": \\\"object\\\",\"\n    \"        \\\"properties\\\": {\"\n    \"            \\\"location\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"City, State, Country\\\"}\"\n    \"        },\"\n    \"        \\\"required\\\": [\\\"location\\\"]\"\n    \"    }\"\n    \"}}]\";\n\nconst char* messages = \"[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What's the weather in Paris?\\\"}]\";\n\nchar response[4096];\nint result = cactus_complete(model, messages, response, sizeof(response),\n                             NULL, tools, NULL, NULL);\nprintf(\"Response: %s\\n\", response);\n</code></pre>"},{"location":"docs/cactus_engine/#computing-similarity-with-embeddings","title":"Computing Similarity with Embeddings","text":"<pre><code>float compute_cosine_similarity(cactus_model_t model, const char* text1, const char* text2) {\n    float embeddings1[2048], embeddings2[2048];\n    size_t dim1, dim2;\n\n    cactus_embed(model, text1, embeddings1, sizeof(embeddings1), &amp;dim1, true);\n    cactus_embed(model, text2, embeddings2, sizeof(embeddings2), &amp;dim2, true);\n\n    // with normalized embeddings, cosine similarity = dot product\n    float dot_product = 0.0f;\n    for (size_t i = 0; i &lt; dim1; i++) {\n        dot_product += embeddings1[i] * embeddings2[i];\n    }\n    return dot_product;\n}\n\nfloat similarity = compute_cosine_similarity(embed_model,\n    \"The cat sat on the mat\", \"A feline rested on the rug\");\nprintf(\"Similarity: %.4f\\n\", similarity);\n</code></pre>"},{"location":"docs/cactus_engine/#audio-transcription-with-whisper","title":"Audio Transcription with Whisper","text":"<pre><code>#include \"cactus_ffi.h\"\n#include &lt;stdio.h&gt;\n\nvoid transcription_callback(const char* token, uint32_t token_id, void* user_data) {\n    printf(\"%s\", token);\n    fflush(stdout);\n}\n\nint main() {\n    cactus_model_t whisper = cactus_init(\"path/to/whisper-small\", NULL);\n    if (!whisper) return -1;\n\n    char response[32768];\n    int result = cactus_transcribe(whisper, \"meeting.wav\", NULL,\n                                    response, sizeof(response), NULL,\n                                    transcription_callback, NULL, NULL, 0);\n    printf(\"\\n\\nFull response: %s\\n\", response);\n\n    cactus_destroy(whisper);\n    return 0;\n}\n</code></pre>"},{"location":"docs/cactus_engine/#multimodal-retrieval","title":"Multimodal Retrieval","text":"<pre><code>#include \"cactus_ffi.h\"\n#include &lt;math.h&gt;\n\nint find_similar_image(cactus_model_t model, const char* query,\n                       const char** image_paths, int num_images) {\n    float query_embed[1024];\n    size_t query_dim;\n    cactus_embed(model, query, query_embed, sizeof(query_embed), &amp;query_dim, true);\n\n    float best_score = -1.0f;\n    int best_idx = -1;\n\n    for (int i = 0; i &lt; num_images; i++) {\n        float img_embed[1024];\n        size_t img_dim;\n        cactus_image_embed(model, image_paths[i], img_embed, sizeof(img_embed), &amp;img_dim);\n\n        float dot = 0, norm_q = 0, norm_i = 0;\n        for (size_t j = 0; j &lt; query_dim; j++) {\n            dot += query_embed[j] * img_embed[j];\n            norm_q += query_embed[j] * query_embed[j];\n            norm_i += img_embed[j] * img_embed[j];\n        }\n        float score = dot / (sqrtf(norm_q) * sqrtf(norm_i));\n\n        if (score &gt; best_score) {\n            best_score = score;\n            best_idx = i;\n        }\n    }\n    return best_idx;\n}\n</code></pre>"},{"location":"docs/cactus_engine/#supported-model-types","title":"Supported Model Types","text":"Model Type Text Vision Audio Embeddings Description Qwen \u2713 - - \u2713 Qwen/Qwen2/Qwen3 language models Gemma \u2713 - - \u2713 Google Gemma models LFM2 \u2713 \u2713 - \u2713 Liquid Foundation Models Smol \u2713 - - \u2713 SmolLM compact models Nomic - - - \u2713 Nomic embedding models Whisper - - \u2713 \u2713 OpenAI Whisper transcription Moonshine - - \u2713 \u2713 UsefulSensors Moonshine transcription Siglip2 - \u2713 - \u2713 Vision encoder for embeddings"},{"location":"docs/cactus_engine/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>CACTUS_KV_WINDOW_SIZE</code> 512 Sliding window size for KV cache <code>CACTUS_KV_SINK_SIZE</code> 4 Number of attention sink tokens to preserve <p>Example: <pre><code>export CACTUS_KV_WINDOW_SIZE=1024\nexport CACTUS_KV_SINK_SIZE=8\n./my_app\n</code></pre></p>"},{"location":"docs/cactus_engine/#best-practices","title":"Best Practices","text":"<ol> <li>Always Check Return Values: Functions return negative values on error</li> <li>Buffer Sizes: Use large response buffers (8192+ bytes recommended)</li> <li>Memory Management: Always call <code>cactus_destroy()</code> when done</li> <li>Thread Safety: Each model instance should be used from a single thread</li> <li>Context Management: Use <code>cactus_reset()</code> between unrelated conversations</li> <li>Streaming: Implement callbacks for better user experience with long generations</li> <li>Reuse Models: Initialize once, use multiple times for efficiency</li> </ol>"},{"location":"docs/cactus_engine/#error-handling","title":"Error Handling","text":"<p>Most functions return: - Positive values or 0 on success - Negative values on error</p> <p>Common error scenarios: - Invalid model path - Insufficient buffer size - Malformed JSON input - Unsupported operation for model type - Out of memory</p>"},{"location":"docs/cactus_engine/#performance-tips","title":"Performance Tips","text":"<ol> <li>Reuse Model Instances: Initialize once, use multiple times</li> <li>Streaming for UX: Use callbacks for responsive user interfaces</li> <li>Early Stopping: Use <code>cactus_stop()</code> to avoid unnecessary generation</li> <li>Batch Embeddings: When possible, process multiple texts in sequence without resetting</li> <li>KV Cache Tuning: Adjust <code>CACTUS_KV_WINDOW_SIZE</code> based on your context needs</li> </ol>"},{"location":"docs/cactus_engine/#see-also","title":"See Also","text":"<ul> <li>Cactus Graph API \u2014 Low-level computational graph for custom tensor operations</li> <li>Cactus Index API \u2014 On-device vector database for RAG applications</li> <li>Fine-tuning Guide \u2014 Deploy Unsloth LoRA fine-tunes to mobile</li> <li>Runtime Compatibility \u2014 Weight versioning across releases</li> <li>Python SDK \u2014 Python bindings for the Engine API</li> <li>Swift SDK \u2014 Swift bindings with async/await support</li> <li>Kotlin/Android SDK \u2014 Kotlin Multiplatform bindings</li> <li>Flutter SDK \u2014 Dart FFI bindings for mobile apps</li> <li>Rust SDK \u2014 Rust FFI bindings via bindgen</li> </ul>"},{"location":"docs/cactus_graph/","title":"Cactus Graph API Documentation","text":"<p>The Cactus Graph API provides a computational graph framework for building and executing tensor operations. It supports multiple precision types, broadcasting, and optimized execution for neural network inference.</p>"},{"location":"docs/cactus_graph/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Setup</li> <li>Core Concepts</li> <li>Getting Started</li> <li>Tensor Operations</li> <li>Advanced Features</li> <li>Complete Examples</li> <li>Best Practices</li> </ul>"},{"location":"docs/cactus_graph/#setup","title":"Setup","text":"<p>Before using the Cactus Graph API, set up your development environment:</p> <pre><code># Setup the environment and install dependencies\n./setup\n\n# Build the Cactus library\ncactus build\n\n# Run tests to verify everything works\ncactus test\n</code></pre>"},{"location":"docs/cactus_graph/#core-concepts","title":"Core Concepts","text":""},{"location":"docs/cactus_graph/#precision-types","title":"Precision Types","text":"<p>The framework supports three precision types for tensors:</p> <pre><code>enum class Precision {\n    INT4,\n    INT8,\n    FP16\n};\n</code></pre> <p>Note: INT4 tensors use packed storage (2 values per byte) and automatically unpack to INT8 for computation.</p>"},{"location":"docs/cactus_graph/#graph-construction","title":"Graph Construction","text":"<p>The <code>CactusGraph</code> class manages the computational graph:</p> <pre><code>CactusGraph graph;\nsize_t input = graph.input({2, 3}, Precision::INT8);\nsize_t result = graph.add(input, another_input);\ngraph.execute();\nvoid* output = graph.get_output(result);\n</code></pre>"},{"location":"docs/cactus_graph/#test-fixtures","title":"Test Fixtures","text":"<p>For testing, use the provided fixtures that handle memory management:</p> <pre><code>TestUtils::Int8TestFixture fixture(\"My Test\");\nTestUtils::FloatTestFixture fixture(\"Float Test\");\n</code></pre>"},{"location":"docs/cactus_graph/#getting-started","title":"Getting Started","text":""},{"location":"docs/cactus_graph/#basic-example","title":"Basic Example","text":"<pre><code>#include \"cactus/graph/graph.h\"\n\nCactusGraph graph;\nsize_t a = graph.input({4}, Precision::INT8);\nsize_t b = graph.input({4}, Precision::INT8);\nsize_t sum = graph.add(a, b);\n\nstd::vector&lt;int8_t&gt; data_a = {1, 2, 3, 4};\nstd::vector&lt;int8_t&gt; data_b = {5, 6, 7, 8};\ngraph.set_input(a, data_a.data(), Precision::INT8);\ngraph.set_input(b, data_b.data(), Precision::INT8);\n\ngraph.execute();\nint8_t* result = static_cast&lt;int8_t*&gt;(graph.get_output(sum)); // [6, 8, 10, 12]\n</code></pre>"},{"location":"docs/cactus_graph/#tensor-operations","title":"Tensor Operations","text":""},{"location":"docs/cactus_graph/#basic-arithmetic","title":"Basic Arithmetic","text":""},{"location":"docs/cactus_graph/#element-wise-operations","title":"Element-wise Operations","text":"<pre><code>size_t add_result = graph.add(a, b);           // a + b\nsize_t sub_result = graph.subtract(a, b);      // a - b\nsize_t mul_result = graph.multiply(a, b);      // a * b\nsize_t div_result = graph.divide(a, b);        // a / b\n</code></pre>"},{"location":"docs/cactus_graph/#scalar-operations","title":"Scalar Operations","text":"<pre><code>size_t scalar_add = graph.scalar_add(input, 5.0f);        // input + 5\nsize_t scalar_sub = graph.scalar_subtract(input, 2.0f);   // input - 2\nsize_t scalar_mul = graph.scalar_multiply(input, 3.0f);   // input * 3\nsize_t scalar_div = graph.scalar_divide(input, 2.0f);     // input / 2\n</code></pre>"},{"location":"docs/cactus_graph/#mathematical-functions","title":"Mathematical Functions","text":"<pre><code>size_t exp_result = graph.scalar_exp(input);    // e^input\nsize_t sqrt_result = graph.scalar_sqrt(input);  // \u221ainput\nsize_t cos_result = graph.scalar_cos(input);    // cos(input)\nsize_t sin_result = graph.scalar_sin(input);    // sin(input)\nsize_t log_result = graph.scalar_log(input);    // ln(input)\n</code></pre>"},{"location":"docs/cactus_graph/#matrix-operations","title":"Matrix Operations","text":""},{"location":"docs/cactus_graph/#matrix-multiplication","title":"Matrix Multiplication","text":"<pre><code>// Standard matmul: (2,3) x (3,4) = (2,4)\nsize_t a = graph.input({2, 3}, Precision::FP16);\nsize_t b = graph.input({3, 4}, Precision::FP16);\nsize_t result = graph.matmul(a, b);\n\n// With pre-transposed right-hand side\nsize_t result = graph.matmul(a, b, true);\n</code></pre>"},{"location":"docs/cactus_graph/#transpose","title":"Transpose","text":"<pre><code>size_t transposed = graph.transpose(input); // (2,3) -&gt; (3,2)\n</code></pre>"},{"location":"docs/cactus_graph/#reshape","title":"Reshape","text":"<pre><code>size_t reshaped = graph.reshape(input, {6, 1}); // (2,3) -&gt; (6,1)\n</code></pre>"},{"location":"docs/cactus_graph/#reduction-operations","title":"Reduction Operations","text":"<pre><code>size_t sum_all = graph.sum(input, -1);   // -1 for all elements\nsize_t sum_axis0 = graph.sum(input, 0);\nsize_t mean_all = graph.mean(input, -1);\nsize_t var = graph.variance(input, axis);\nsize_t min_val = graph.min(input, axis);\nsize_t max_val = graph.max(input, axis);\n</code></pre>"},{"location":"docs/cactus_graph/#neural-network-operations","title":"Neural Network Operations","text":""},{"location":"docs/cactus_graph/#layer-normalization","title":"Layer Normalization","text":"<pre><code>size_t weight = graph.input({hidden_size}, Precision::FP16);\nsize_t bias = graph.input({hidden_size}, Precision::FP16);\nsize_t normalized = graph.layernorm(input, weight, bias, 1e-5f);\n</code></pre>"},{"location":"docs/cactus_graph/#rms-normalization","title":"RMS Normalization","text":"<pre><code>size_t weight = graph.input({hidden_size}, Precision::FP16);\nsize_t normalized = graph.rms_norm(input, weight, 1e-5f);\n</code></pre>"},{"location":"docs/cactus_graph/#softmax","title":"Softmax","text":"<pre><code>size_t softmax_result = graph.softmax(input, -1);\n</code></pre>"},{"location":"docs/cactus_graph/#attention-mechanism","title":"Attention Mechanism","text":"<pre><code>size_t attention_out = graph.attention(query, key, value, scale);\nsize_t attention_out = graph.attention(query, key, value, scale, position_offset);\nsize_t attention_out = graph.attention(query, key, value, scale, position_offset, window_size);\n</code></pre>"},{"location":"docs/cactus_graph/#rotary-position-embedding-rope","title":"Rotary Position Embedding (RoPE)","text":"<pre><code>size_t rope_output = graph.rope(input, theta, position_offset);\n</code></pre>"},{"location":"docs/cactus_graph/#activation-functions","title":"Activation Functions","text":"<pre><code>size_t silu_out = graph.silu(input);\nsize_t gelu_out = graph.gelu(input);\n</code></pre>"},{"location":"docs/cactus_graph/#indexing-and-gathering","title":"Indexing and Gathering","text":""},{"location":"docs/cactus_graph/#gather-operation","title":"Gather Operation","text":"<pre><code>size_t embeddings = graph.input({vocab_size, embed_dim}, Precision::FP16);\nsize_t indices = graph.input({batch_size, seq_len}, Precision::INT8);\nsize_t gathered = graph.gather(embeddings, indices);\n</code></pre>"},{"location":"docs/cactus_graph/#embedding-lookup","title":"Embedding Lookup","text":"<pre><code>size_t embedded = graph.embedding(embedding_tensor, indices);\nsize_t embedded = graph.embedding(\"embeddings.bin\", indices); // memory-mapped\n</code></pre>"},{"location":"docs/cactus_graph/#memory-mapped-weights","title":"Memory-Mapped Weights","text":"<pre><code>size_t mmap_embed = graph.mmap_embeddings(\"embeddings.bin\");\nsize_t weights = graph.mmap_weights(\"model_weights.bin\");\n</code></pre>"},{"location":"docs/cactus_graph/#advanced-operations","title":"Advanced Operations","text":""},{"location":"docs/cactus_graph/#concatenation","title":"Concatenation","text":"<pre><code>size_t concatenated = graph.concat(tensor1, tensor2, axis);\n</code></pre>"},{"location":"docs/cactus_graph/#slicing","title":"Slicing","text":"<pre><code>size_t sliced = graph.slice(input, axis, start, length);\n</code></pre>"},{"location":"docs/cactus_graph/#indexing","title":"Indexing","text":"<pre><code>size_t indexed = graph.index(input, index_value, dimension);\n</code></pre>"},{"location":"docs/cactus_graph/#top-k-selection","title":"Top-K Selection","text":"<pre><code>size_t topk_values = graph.topk(input, k);\n</code></pre>"},{"location":"docs/cactus_graph/#sampling","title":"Sampling","text":"<pre><code>size_t sampled = graph.sample(logits, temperature, top_p, top_k);\n</code></pre>"},{"location":"docs/cactus_graph/#advanced-features","title":"Advanced Features","text":""},{"location":"docs/cactus_graph/#broadcasting","title":"Broadcasting","text":"<p>The framework automatically handles broadcasting for compatible shapes:</p> <pre><code>size_t tensor = graph.input({2, 3}, Precision::INT8);\nsize_t scalar = graph.input({1}, Precision::INT8);\nsize_t result = graph.add(tensor, scalar);  // {1} -&gt; {2,3}\n\nsize_t a = graph.input({2, 3}, Precision::INT8);\nsize_t b = graph.input({2, 1}, Precision::INT8);\nsize_t result = graph.add(a, b);  // {2,1} -&gt; {2,3}\n\nsize_t a = graph.input({2, 2, 3}, Precision::INT8);\nsize_t b = graph.input({2, 3}, Precision::INT8);\nsize_t result = graph.add(a, b);  // {2,3} -&gt; {2,2,3}\n</code></pre>"},{"location":"docs/cactus_graph/#precision-conversion","title":"Precision Conversion","text":"<pre><code>size_t int8_tensor = graph.input({4}, Precision::INT8);\nsize_t fp16_tensor = graph.precision_cast(int8_tensor, Precision::FP16);\ngraph.set_quantization_scale(node_id, scale);\n</code></pre>"},{"location":"docs/cactus_graph/#graph-persistence","title":"Graph Persistence","text":""},{"location":"docs/cactus_graph/#saving-nodes","title":"Saving Nodes","text":"<pre><code>GraphFile::save_node(graph, node_id, \"output.bin\");\n</code></pre>"},{"location":"docs/cactus_graph/#loading-nodes","title":"Loading Nodes","text":"<pre><code>CactusGraph new_graph;\nauto loaded = GraphFile::load_into_graph(new_graph, \"output.bin\");\nsize_t node_id = loaded.node_id;\nstd::vector&lt;size_t&gt; shape = loaded.shape;\nPrecision precision = loaded.precision;\n</code></pre>"},{"location":"docs/cactus_graph/#graph-management","title":"Graph Management","text":""},{"location":"docs/cactus_graph/#execution","title":"Execution","text":"<pre><code>graph.execute();\ngraph.execute(\"profile_output.json\"); // with profiling\n</code></pre>"},{"location":"docs/cactus_graph/#reset-operations","title":"Reset Operations","text":"<pre><code>graph.hard_reset(); // clear all nodes and buffers\ngraph.soft_reset(); // clear only buffers, keep graph structure\n</code></pre>"},{"location":"docs/cactus_graph/#complete-examples","title":"Complete Examples","text":""},{"location":"docs/cactus_graph/#building-a-simple-neural-network-layer","title":"Building a Simple Neural Network Layer","text":"<pre><code>CactusGraph graph;\n\nsize_t input = graph.input({2, 4}, Precision::FP16);\nsize_t weight = graph.input({4, 8}, Precision::FP16);\nsize_t bias = graph.input({8}, Precision::FP16);\n\nsize_t linear = graph.matmul(input, weight);\nsize_t with_bias = graph.add(linear, bias);\nsize_t activated = graph.gelu(with_bias);\n\nsize_t ln_weight = graph.input({8}, Precision::FP16);\nsize_t ln_bias = graph.input({8}, Precision::FP16);\nsize_t output = graph.layernorm(activated, ln_weight, ln_bias);\n</code></pre>"},{"location":"docs/cactus_graph/#implementing-multi-head-attention","title":"Implementing Multi-Head Attention","text":"<pre><code>CactusGraph graph;\n\nsize_t hidden_dim = 512;\nsize_t num_heads = 8;\nsize_t head_dim = hidden_dim / num_heads;\nsize_t seq_len = 32;\n\nsize_t input = graph.input({1, seq_len, hidden_dim}, Precision::FP16);\nsize_t q_weight = graph.input({hidden_dim, hidden_dim}, Precision::FP16);\nsize_t k_weight = graph.input({hidden_dim, hidden_dim}, Precision::FP16);\nsize_t v_weight = graph.input({hidden_dim, hidden_dim}, Precision::FP16);\n\nsize_t query = graph.matmul(input, q_weight);\nsize_t key = graph.matmul(input, k_weight);\nsize_t value = graph.matmul(input, v_weight);\n\nquery = graph.reshape(query, {1, seq_len, num_heads, head_dim});\nkey = graph.reshape(key, {1, seq_len, num_heads, head_dim});\nvalue = graph.reshape(value, {1, seq_len, num_heads, head_dim});\n\nfloat scale = 1.0f / sqrt(head_dim);\nsize_t attention_out = graph.attention(query, key, value, scale);\n</code></pre>"},{"location":"docs/cactus_graph/#working-with-embeddings","title":"Working with Embeddings","text":"<pre><code>CactusGraph graph;\n\nsize_t vocab_size = 50000;\nsize_t embed_dim = 768;\nsize_t tokens = graph.input({2, 10}, Precision::INT8);\n\nsize_t embed_table = graph.input({vocab_size, embed_dim}, Precision::FP16);\nsize_t embeddings = graph.gather(embed_table, tokens);\n\n// or memory-mapped for large models\nsize_t mmap_table = graph.mmap_embeddings(\"vocab_embeddings.bin\");\nsize_t embeddings = graph.gather(mmap_table, tokens);\n\nsize_t pos_embed = graph.input({1, 10, embed_dim}, Precision::FP16);\nsize_t final_embed = graph.add(embeddings, pos_embed);\n</code></pre>"},{"location":"docs/cactus_graph/#similarity-computation","title":"Similarity Computation","text":"<pre><code>TestUtils::FloatTestFixture fixture(\"Similarity\");\n\nsize_t text1 = fixture.create_input({1, 768}, Precision::FP16);\nsize_t text2 = fixture.create_input({1, 768}, Precision::FP16);\n\n// L2 norms\nsize_t norm1 = fixture.graph().scalar_sqrt(\n    fixture.graph().sum(fixture.graph().multiply(text1, text1), -1));\nsize_t norm2 = fixture.graph().scalar_sqrt(\n    fixture.graph().sum(fixture.graph().multiply(text2, text2), -1));\n\n// cosine similarity = dot(a,b) / (norm(a) * norm(b))\nsize_t dot_product = fixture.graph().sum(fixture.graph().multiply(text1, text2), -1);\nsize_t similarity = fixture.graph().divide(dot_product, fixture.graph().multiply(norm1, norm2));\n</code></pre>"},{"location":"docs/cactus_graph/#best-practices","title":"Best Practices","text":""},{"location":"docs/cactus_graph/#memory-management","title":"Memory Management","text":"<ol> <li>Use appropriate precision: INT4/INT8 for memory efficiency, FP16 for accuracy</li> <li>Memory-map large tensors: Use <code>mmap_embeddings()</code> for vocabulary tables</li> <li>Reset graphs: Call <code>hard_reset()</code> when switching between different models</li> <li>External buffers: Use <code>set_external_input()</code> to avoid copying large inputs</li> </ol>"},{"location":"docs/cactus_graph/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Batch operations: Process multiple samples together</li> <li>Pre-transpose weights: Use <code>pretransposed_rhs=true</code> for matmul when possible</li> <li>Fused operations: The framework automatically fuses compatible operations</li> <li>Backend selection: Use NPU backend for supported operations:    <pre><code>size_t result = graph.matmul(a, b, false, ComputeBackend::NPU);\n</code></pre></li> </ol>"},{"location":"docs/cactus_graph/#graph-construction_1","title":"Graph Construction","text":"<ol> <li>Build once, execute many: Construct the graph once, run with different inputs</li> <li>Validate shapes: Ensure tensor shapes are compatible before operations</li> <li>Handle broadcasts: Be aware of automatic broadcasting rules</li> <li>Profile execution: Use <code>execute(\"profile.json\")</code> to identify bottlenecks</li> </ol>"},{"location":"docs/cactus_graph/#testing","title":"Testing","text":"<ol> <li>Use test fixtures: Leverage provided fixtures for automatic cleanup</li> <li>Verify outputs: Use <code>verify_output()</code> methods for tolerance-based comparison</li> <li>Test edge cases: Include tests for broadcasting, empty tensors, and large inputs</li> <li>Check precision: Test operations with different precision types</li> </ol>"},{"location":"docs/cactus_graph/#error-handling","title":"Error Handling","text":"<pre><code>try {\n    CactusGraph graph;\n    // ... build and execute graph\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Graph error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"docs/cactus_graph/#common-patterns","title":"Common Patterns","text":""},{"location":"docs/cactus_graph/#sequential-processing","title":"Sequential Processing","text":"<pre><code>CactusGraph graph;\nsize_t x = graph.input({batch, dim}, Precision::FP16);\nx = graph.linear(x, weight1, bias1);\nx = graph.gelu(x);\nx = graph.layernorm(x, ln_weight1, ln_bias1);\nx = graph.linear(x, weight2, bias2);\n</code></pre>"},{"location":"docs/cactus_graph/#residual-connections","title":"Residual Connections","text":"<pre><code>size_t input = graph.input({batch, dim}, Precision::FP16);\nsize_t processed = graph.matmul(input, weight);\nprocessed = graph.gelu(processed);\nsize_t output = graph.add(input, processed);\n</code></pre>"},{"location":"docs/cactus_graph/#multi-path-processing","title":"Multi-Path Processing","text":"<pre><code>size_t input = graph.input({batch, dim}, Precision::FP16);\nsize_t path1 = graph.matmul(input, weight1);\npath1 = graph.silu(path1);\nsize_t path2 = graph.matmul(input, weight2);\nsize_t output = graph.multiply(path1, path2);\n</code></pre>"},{"location":"docs/cactus_graph/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 High-level inference API built on top of Cactus Graph</li> <li>Cactus Index API \u2014 On-device vector database for embedding storage and search</li> <li>Runtime Compatibility \u2014 Weight versioning across releases</li> </ul>"},{"location":"docs/cactus_index/","title":"Cactus Index FFI Documentation","text":"<p>The Cactus Index provides a clean C FFI (Foreign Function Interface) for integrating a vector database into various applications. This documentation covers all available functions, their parameters, and usage examples.</p>"},{"location":"docs/cactus_index/#getting-started","title":"Getting Started","text":"<p>The index uses memory-mapped files: - <code>index.bin</code>: Embeddings (FP16) and metadata pointers - <code>data.bin</code>: Document content and metadata (UTF-8)</p> <p>All embeddings are automatically normalized to unit length</p>"},{"location":"docs/cactus_index/#types","title":"Types","text":""},{"location":"docs/cactus_index/#cactus_index_t","title":"<code>cactus_index_t</code>","text":"<p>An opaque pointer type representing an index instance. This handle is used throughout the API to reference a specific index.</p> <pre><code>typedef void* cactus_index_t;\n</code></pre>"},{"location":"docs/cactus_index/#core-functions","title":"Core Functions","text":""},{"location":"docs/cactus_index/#cactus_index_init","title":"<code>cactus_index_init</code>","text":"<p>Initializes or opens an index with specified embedding dimension.</p> <pre><code>cactus_index_t cactus_index_init(\n    const char* index_dir,\n    size_t embedding_dim\n);\n</code></pre> <p>Parameters: - <code>index_dir</code>: Directory path where index files will be stored - <code>embedding_dim</code>: Dimension of embeddings (must match for existing index)</p> <p>Returns: Index handle on success, NULL on failure</p> <p>Example: <pre><code>cactus_index_t index = cactus_index_init(\"./my_index\", 768);\nif (!index) {\n    fprintf(stderr, \"Failed to initialize index\\n\");\n    return -1;\n}\n</code></pre></p>"},{"location":"docs/cactus_index/#cactus_index_add","title":"<code>cactus_index_add</code>","text":"<p>Adds documents to the index.</p> <pre><code>int cactus_index_add(\n    cactus_index_t index,\n    const int* ids,\n    const char** documents,\n    const char** metadatas,\n    const float** embeddings,\n    size_t count,\n    size_t embedding_dim\n);\n</code></pre> <p>Parameters: - <code>index</code>: Index handle from <code>cactus_index_init</code> (required) - <code>ids</code>: Array of unique document IDs (required) - <code>documents</code>: Array of document content strings (required, UTF-8) - <code>metadatas</code>: Array of metadata strings (optional, can be NULL; UTF-8) - <code>embeddings</code>: Array of pointers to embedding vectors. (required, none can be NULL) - <code>count</code>: Number of documents to add (must be &gt; 0) - <code>embedding_dim</code>: Dimension of embeddings (must match index, must be &gt; 0)</p> <p>Returns: 0 on success, -1 on error</p> <p>Constraints: - Document IDs must be unique integers - Content max 65535 bytes per document - Metadata max 65535 bytes per document - Embedding dimension must match index dimension from init - <code>ids</code>, <code>documents</code>, and <code>embeddings</code> arrays must be non-null - Each individual <code>embeddings[i]</code> pointer must be non-null - Individual <code>documents[i]</code> can be NULL (stored as empty string) - <code>metadatas</code> array can be NULL, or individual <code>metadatas[i]</code> can be NULL</p> <p>Example: <pre><code>int ids[] = {1, 2};\nconst char* docs[] = {\"AI is transforming technology\", \"Machine learning enables predictions\"};\nconst char* metas[] = {\"{\\\"source\\\":\\\"wiki\\\"}\", \"{\\\"source\\\":\\\"blog\\\"}\"};\n\nfloat emb1[768] = {0.1, 0.2, 0.3, /* ... */};\nfloat emb2[768] = {0.4, 0.5, 0.6, /* ... */};\nconst float* embeddings[] = {emb1, emb2};\n\nint result = cactus_index_add(index, ids, docs, metas, embeddings, 2, 768);\nif (result != 0) {\n    fprintf(stderr, \"Failed to add documents: %s\\n\", cactus_get_last_error());\n}\n</code></pre></p> <p>Example without metadata: <pre><code>int ids[] = {1, 2};\nconst char* docs[] = {\"Document one\", \"Document two\"};\n\nfloat emb1[768] = {0.1, 0.2, 0.3, /* ... */};\nfloat emb2[768] = {0.4, 0.5, 0.6, /* ... */};\nconst float* embeddings[] = {emb1, emb2};\n\n// Pass NULL for metadatas\nint result = cactus_index_add(index, ids, docs, NULL, embeddings, 2, 768);\n</code></pre></p>"},{"location":"docs/cactus_index/#cactus_index_delete","title":"<code>cactus_index_delete</code>","text":"<p>Soft deletes documents. Space reclaimed via <code>cactus_index_compact</code>.</p> <pre><code>int cactus_index_delete(\n    cactus_index_t index,\n    const int* ids,\n    size_t ids_count\n);\n</code></pre> <p>Parameters: - <code>index</code>: Index handle from <code>cactus_index_init</code> (required) - <code>ids</code>: Array of document IDs to delete (required) - <code>ids_count</code>: Number of document IDs (must be &gt; 0)</p> <p>Returns: 0 on success, -1 on error</p> <p>Example: <pre><code>int ids[] = {1, 2, 3};\nint result = cactus_index_delete(index, ids, 3);\nif (result != 0) {\n    fprintf(stderr, \"Failed to delete documents: %s\\n\", cactus_get_last_error());\n}\n</code></pre></p>"},{"location":"docs/cactus_index/#cactus_index_get","title":"<code>cactus_index_get</code>","text":"<p>Retrieves documents by IDs. Fetch only the fields you need by passing NULL for unused buffers.</p> <pre><code>int cactus_index_get(\n    cactus_index_t index,\n    const int* ids,\n    size_t ids_count,\n    char** document_buffers,\n    size_t* document_buffer_sizes,\n    char** metadata_buffers,\n    size_t* metadata_buffer_sizes,\n    float** embedding_buffers,\n    size_t* embedding_buffer_sizes\n);\n</code></pre> <p>Parameters: - <code>index</code>: Index handle from <code>cactus_index_init</code> (required) - <code>ids</code>: Array of document IDs to retrieve (required) - <code>ids_count</code>: Number of document IDs (must be &gt; 0) - <code>document_buffers</code>: Array of pre-allocated buffers for document content (optional, can be NULL) - <code>document_buffer_sizes</code>: Array serving dual purpose (required if <code>document_buffers</code> is not NULL):   - Input: Initial capacity of each buffer in bytes   - Output: Actual size of data written to each buffer (including null terminator) - <code>metadata_buffers</code>: Array of pre-allocated buffers for metadata (optional, can be NULL) - <code>metadata_buffer_sizes</code>: Array serving dual purpose (required if <code>metadata_buffers</code> is not NULL):   - Input: Initial capacity of each buffer in bytes   - Output: Actual size of data written to each buffer (including null terminator) - <code>embedding_buffers</code>: Array of pre-allocated buffers for embeddings (optional, can be NULL) - <code>embedding_buffer_sizes</code>: Array for embedding buffer sizes (required if <code>embedding_buffers</code> is not NULL)   - Input: Capacity in number of floats   - Output: Actual number of floats written</p> <p>Returns: 0 on success, -1 on error (e.g., buffer too small, no data copied)</p> <p>Example - Retrieve all fields: <pre><code>int ids[] = {1, 2};\nchar* docs[2];\nchar* metas[2];\nfloat* embs[2];\n\n// Allocate buffers\nfor (int i = 0; i &lt; 2; i++) {\n    docs[i] = malloc(65536);\n    metas[i] = malloc(65536);\n    embs[i] = malloc(768 * sizeof(float));\n}\n\nsize_t doc_sizes[2] = {65536, 65536};\nsize_t meta_sizes[2] = {65536, 65536};\nsize_t emb_sizes[2] = {768, 768};  // Number of floats\n\nint result = cactus_index_get(index, ids, 2,\n                               docs, doc_sizes,\n                               metas, meta_sizes,\n                               embs, emb_sizes);\n\nif (result == 0) {\n    printf(\"Retrieved documents successfully\\n\");\n    for (int i = 0; i &lt; 2; i++) {\n        printf(\"Doc %d (%zu bytes): %s\\n\", ids[i], doc_sizes[i], docs[i]);\n        printf(\"Embedding dim: %zu floats\\n\", emb_sizes[i]);\n    }\n}\n</code></pre></p> <p>Example - Retrieve only documents (no metadata or embeddings): <pre><code>int ids[] = {1, 2, 3};\nchar* docs[3];\n\nfor (int i = 0; i &lt; 3; i++) {\n    docs[i] = malloc(65536);\n}\n\nsize_t doc_sizes[3] = {65536, 65536, 65536};\n\n// Pass NULL for metadata and embeddings\nint result = cactus_index_get(index, ids, 3,\n                               docs, doc_sizes,\n                               NULL, NULL,  // No metadata\n                               NULL, NULL); // No embeddings\n\nif (result == 0) {\n    for (int i = 0; i &lt; 3; i++) {\n        printf(\"%s\\n\", docs[i]);\n    }\n}\n</code></pre></p>"},{"location":"docs/cactus_index/#cactus_index_query","title":"<code>cactus_index_query</code>","text":"<p>Similarity search using cosine similarity. Results sorted by score (highest first).</p> <pre><code>int cactus_index_query(\n    cactus_index_t index,\n    const float** embeddings,\n    size_t embeddings_count,\n    size_t embedding_dim,\n    const char* options_json,\n    int** id_buffers,\n    size_t* id_buffer_sizes,\n    float** score_buffers,\n    size_t* score_buffer_sizes\n);\n</code></pre> <p>Parameters: - <code>index</code>: Index handle from <code>cactus_index_init</code> (required) - <code>embeddings</code>: Array of query embedding pointers (required, none can be NULL) - <code>embeddings_count</code>: Number of query embeddings (must be &gt; 0) - <code>embedding_dim</code>: Dimension of embeddings (must match index, must be &gt; 0) - <code>options_json</code>: JSON string with query options (optional, can be NULL for defaults) - <code>id_buffers</code>: Array of pre-allocated buffers for result document IDs (required, one buffer per query) - <code>id_buffer_sizes</code>: Array serving dual purpose (required):   - Input: Maximum capacity of each buffer (number of results)   - Output: Actual number of results per query - <code>score_buffers</code>: Array of pre-allocated buffers for result scores (required, one buffer per query) - <code>score_buffer_sizes</code>: Array serving dual purpose (required):   - Input: Maximum capacity of each buffer (number of results)   - Output: Actual number of results per query</p> <p>Options JSON Format: <pre><code>{\n    \"top_k\": 10,\n    \"score_threshold\": 0.7\n}\n</code></pre></p> <p>Defaults: <code>top_k</code>: 10, <code>score_threshold</code>: -1.0 (no filtering)</p> <p>Returns: 0 on success, -1 on error (if buffers too small, no data copied)</p> <p>Example <pre><code>float query1[768] = {/* ... */};\nfloat query2[768] = {/* ... */};\nconst float* queries[] = {query1, query2};\n\n// Allocate result buffers (2 queries, max 5 results each)\nint* ids[2];\nfloat* scores[2];\nfor (int i = 0; i &lt; 2; i++) {\n    ids[i] = (int*)malloc(5 * sizeof(int));\n    scores[i] = (float*)malloc(5 * sizeof(float));\n}\n\nsize_t id_sizes[2] = {5, 5};\nsize_t score_sizes[2] = {5, 5};\n\nint result = cactus_index_query(index, queries, 2, 768, \"{\\\"top_k\\\":5}\",\n                                 ids, id_sizes,\n                                 scores, score_sizes);\n\nif (result == 0) {\n    for (int q = 0; q &lt; 2; q++) {\n        printf(\"Query %d: %zu results\\n\", q, id_sizes[q]);\n        for (size_t i = 0; i &lt; id_sizes[q]; i++) {\n            printf(\"  ID: %d, Score: %.3f\\n\", ids[q][i], scores[q][i]);\n        }\n    }\n}\n</code></pre></p>"},{"location":"docs/cactus_index/#cactus_index_compact","title":"<code>cactus_index_compact</code>","text":"<p>Removes deleted documents and reclaims disk space.</p> <pre><code>int cactus_index_compact(cactus_index_t index);\n</code></pre> <p>Parameters: - <code>index</code>: Index handle from <code>cactus_index_init</code> (required)</p> <p>Returns: 0 on success, -1 on error</p> <p>Example: <pre><code>int result = cactus_index_compact(index);\nif (result != 0) {\n    fprintf(stderr, \"Compaction failed: %s\\n\", cactus_get_last_error());\n}\n</code></pre></p>"},{"location":"docs/cactus_index/#cactus_index_destroy","title":"<code>cactus_index_destroy</code>","text":"<p>Releases all resources associated with the index.</p> <pre><code>void cactus_index_destroy(cactus_index_t index);\n</code></pre>"},{"location":"docs/cactus_index/#examples","title":"Examples","text":""},{"location":"docs/cactus_index/#create-and-populate","title":"Create and Populate","text":"<pre><code>cactus_index_t index = cactus_index_init(\"./my_index\", 768);\nif (!index) {\n    fprintf(stderr, \"Failed to initialize index\\n\");\n    return -1;\n}\n\nint ids[] = {1, 2};\nconst char* docs[] = {\n    \"AI is transforming technology\",\n    \"Machine learning enables predictions\"\n};\nconst char* metas[] = {\"{\\\"source\\\":\\\"wiki\\\"}\", \"{\\\"source\\\":\\\"blog\\\"}\"};\n\nfloat emb1[768] = {0.1, 0.2, 0.3, /* ... */};\nfloat emb2[768] = {0.4, 0.5, 0.6, /* ... */};\nconst float* embeddings[] = {emb1, emb2};\n\nint result = cactus_index_add(index, ids, docs, metas, embeddings, 2, 768);\nif (result != 0) {\n    fprintf(stderr, \"Failed to add documents: %s\\n\", cactus_get_last_error());\n}\n\ncactus_index_destroy(index);\n</code></pre>"},{"location":"docs/cactus_index/#similarity-search","title":"Similarity Search","text":"<pre><code>cactus_index_t index = cactus_index_init(\"./my_index\", 768);\nif (!index) {\n    fprintf(stderr, \"Failed to open index: %s\\n\", cactus_get_last_error());\n    return -1;\n}\n\nfloat query_embedding[768] = {0.1, 0.2, 0.3, /* ... */};\nconst float* queries[] = {query_embedding};\n\n// Allocate buffers for 1 query with max 10 results\nint* ids[1];\nfloat* scores[1];\nids[0] = (int*)malloc(10 * sizeof(int));\nscores[0] = (float*)malloc(10 * sizeof(float));\n\nsize_t id_sizes[1] = {10};\nsize_t score_sizes[1] = {10};\n\nint result = cactus_index_query(index, queries, 1, 768,\n                                 \"{\\\"top_k\\\":10,\\\"score_threshold\\\":0.7}\",\n                                 ids, id_sizes,\n                                 scores, score_sizes);\n\nif (result == 0) {\n    printf(\"Found %zu results:\\n\", id_sizes[0]);\n    for (size_t i = 0; i &lt; id_sizes[0]; i++) {\n        printf(\"  ID: %d, Score: %.3f\\n\", ids[0][i], scores[0][i]);\n    }\n}\n\ncactus_index_destroy(index);\n</code></pre>"},{"location":"docs/cactus_index/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<pre><code>cactus_index_t index = cactus_index_init(\"./my_index\", 768);\nif (!index) return -1;\n\n// Search for relevant documents\nfloat query_embedding[768] = {0.1, 0.2, 0.3, /* ... */};\nconst float* queries[] = {query_embedding};\n\n// Allocate query result buffers\nint* result_ids[1];\nfloat* result_scores[1];\nresult_ids[0] = (int*)malloc(10 * sizeof(int));\nresult_scores[0] = (float*)malloc(10 * sizeof(float));\n\nsize_t id_sizes[1] = {10};\nsize_t score_sizes[1] = {10};\n\nint query_result = cactus_index_query(index, queries, 1, 768,\n                                       \"{\\\"top_k\\\":3,\\\"score_threshold\\\":0.5}\",\n                                       result_ids, id_sizes,\n                                       result_scores, score_sizes);\n\nif (query_result == 0) {\n    // Allocate buffers for retrieving documents\n    size_t num_results = id_sizes[0];\n    char* docs[10];\n\n    for (size_t i = 0; i &lt; num_results; i++) {\n        docs[i] = (char*)malloc(65536);\n    }\n\n    size_t doc_sizes[10];\n    for (size_t i = 0; i &lt; num_results; i++) {\n        doc_sizes[i] = 65536;\n    }\n\n    // Retrieve only documents (no metadata or embeddings needed for RAG)\n    int get_result = cactus_index_get(index, result_ids[0], num_results,\n                                       docs, doc_sizes,\n                                       NULL, NULL,  // No metadata\n                                       NULL, NULL); // No embeddings\n\n    if (get_result == 0) {\n        // Build context from retrieved documents\n        char context[32768] = \"\";\n        for (size_t i = 0; i &lt; num_results; i++) {\n            strcat(context, docs[i]);\n            strcat(context, \"\\n\\n\");\n        }\n\n        printf(\"Context: %s\\n\", context);\n    }\n}\n\ncactus_index_destroy(index);\n</code></pre>"},{"location":"docs/cactus_index/#delete-and-compact","title":"Delete and Compact","text":"<pre><code>cactus_index_t index = cactus_index_init(\"./my_index\", 768);\nif (!index) return -1;\n\nint ids[] = {1, 3, 5};\nint deleted = cactus_index_delete(index, ids, 3);\nif (deleted == 0) {\n    printf(\"Documents deleted successfully\\n\");\n}\n\n// Compact to reclaim space\nint compact_result = cactus_index_compact(index);\nif (compact_result == 0) {\n    printf(\"Index compacted successfully\\n\");\n}\n\ncactus_index_destroy(index);\n</code></pre>"},{"location":"docs/cactus_index/#migrate-embedding-model","title":"Migrate Embedding Model","text":"<pre><code>// Open old index and create new index with different dimensions\ncactus_index_t old_index = cactus_index_init(\"./old_index\", 768);\ncactus_index_t new_index = cactus_index_init(\"./new_index\", 1536);\n\nif (!old_index || !new_index) {\n    fprintf(stderr, \"Failed to open indexes\\n\");\n    return -1;\n}\n\n// Get all documents from old index\nint all_doc_ids[] = {1, 2, 3, 4, 5};\nint num_docs = 5;\n\n// Allocate buffers for old documents\nchar* old_docs[5];\nchar* old_metas[5];\n\nfor (int i = 0; i &lt; num_docs; i++) {\n    old_docs[i] = malloc(65536);\n    old_metas[i] = malloc(65536);\n}\n\nsize_t doc_sizes[5], meta_sizes[5];\nfor (int i = 0; i &lt; num_docs; i++) {\n    doc_sizes[i] = 65536;\n    meta_sizes[i] = 65536;\n}\n\nint get_result = cactus_index_get(old_index, all_doc_ids, num_docs,\n                                   old_docs, doc_sizes,\n                                   old_metas, meta_sizes,\n                                   NULL, NULL);  // Don't need old embeddings\n\nif (get_result == 0) {\n    // Regenerate embeddings with new model (1536 dimensions)\n    float new_embs[5][1536];\n    const float* new_emb_ptrs[5];\n\n    for (int i = 0; i &lt; num_docs; i++) {\n        // Generate new embedding for old_docs[i]\n        // ... embedding generation code ...\n        new_emb_ptrs[i] = new_embs[i];\n    }\n\n    // Add to new index\n    const char* doc_ptrs[5];\n    const char* meta_ptrs[5];\n    for (int i = 0; i &lt; num_docs; i++) {\n        doc_ptrs[i] = old_docs[i];\n        meta_ptrs[i] = old_metas[i];\n    }\n\n    cactus_index_add(new_index, all_doc_ids, doc_ptrs, meta_ptrs,\n                     new_emb_ptrs, num_docs, 1536);\n}\n\ncactus_index_destroy(old_index);\ncactus_index_destroy(new_index);\n</code></pre>"},{"location":"docs/cactus_index/#best-practices","title":"Best Practices","text":"<ol> <li>Return Values: Check all returns (0 = success, -1 = error)</li> <li>Buffers:</li> <li><code>document_buffer_sizes</code> and <code>metadata_buffer_sizes</code>: bytes</li> <li><code>embedding_buffer_sizes</code>: number of floats (not bytes)</li> <li>Pass NULL for unused buffers in <code>cactus_index_get</code></li> <li>Memory: Always call <code>cactus_index_destroy()</code> when done</li> <li>Thread Safety: One index instance per thread</li> <li>Batching: Add 100-1000 documents per call for best performance</li> <li>Errors: Use <code>cactus_get_last_error()</code> for error details</li> </ol>"},{"location":"docs/cactus_index/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 LLM inference, embeddings (<code>cactus_embed</code>), and RAG query APIs</li> <li>Cactus Graph API \u2014 Low-level computational graph for custom tensor operations</li> <li>Python SDK \u2014 Python bindings with vector index support</li> <li>Swift SDK \u2014 Swift <code>CactusIndex</code> class</li> <li>Kotlin/Android SDK \u2014 Kotlin <code>CactusIndex</code> class</li> <li>Flutter SDK \u2014 Dart <code>CactusIndex</code> class</li> </ul>"},{"location":"docs/compatibility/","title":"Runtime &amp; Weights Compatibility","text":"<p>Some Cactus releases change the internal weight format. When this happens, cached weights from an older version will not load with a newer runtime and must be re-downloaded.</p> <p>Breaking weight changes are called out in the release notes.</p>"},{"location":"docs/compatibility/#how-versioning-works","title":"How Versioning Works","text":"<p>Weights are published to Hugging Face and only re-tagged when they actually change. If a release does not affect the weight format, the previous tag remains \u2014 no new upload.</p> <pre><code>Runtime v1.7  -&gt; weights tagged v1.7 on HF\nRuntime v1.8  -&gt; no new tag (unchanged) - still use v1.7\n...\nRuntime v1.14 -&gt; no new tag - still use v1.7\nRuntime v1.15 -&gt; new tag v1.15 (changed!) - must update\n</code></pre> <p>The rule: use the latest HF weight tag that is \u2264 your runtime version.</p>"},{"location":"docs/compatibility/#checking-compatibility","title":"Checking Compatibility","text":"<ol> <li>Open your model on huggingface.co/Cactus-Compute</li> <li>Click Files and versions \u2192 open branch dropdown from Main</li> <li>Find the latest tag that is \u2264 your runtime version</li> <li>If your local weights use an older tag, re-download them</li> </ol>"},{"location":"docs/compatibility/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 Full inference API reference</li> <li>Fine-tuning Guide \u2014 Convert and deploy custom fine-tunes</li> <li>HuggingFace Weights \u2014 Official Cactus model weights</li> </ul>"},{"location":"docs/finetuning/","title":"Deploying Unsloth Fine-Tunes to Cactus for Phones","text":"<ul> <li>Cactus is an inference engine for mobile devices, macs and ARM chips like Raspberry Pi.</li> <li>At INT8, Cactus runs <code>Qwen3-0.6B</code> and <code>LFM2-1.2B</code> at <code>60-70 toks/sec</code> on iPhone 17 Pro, <code>13-18 toks/sec</code> on budget Pixel 6a.</li> <li>INT4 quantization provides ~50% memory reduction with minimal quality loss.</li> <li>Task-Specific INT8 tunes of <code>Gemma3-270m</code> hit <code>150 toks/sec</code> on iPhone 17 Pro and <code>23 toks/sec</code> on Raspberry Pi. </li> </ul>"},{"location":"docs/finetuning/#quick-start","title":"Quick Start","text":""},{"location":"docs/finetuning/#1-train-google-colab-gpu","title":"1. Train (Google Colab / GPU)","text":"<p>Use the provided notebook or your own Unsloth training script:</p> <pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gemma-3-4b-it\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# ... train with SFTTrainer ...\n\n# Save adapter\nmodel.save_pretrained(\"my-lora-adapter\")\ntokenizer.save_pretrained(\"my-lora-adapter\")\n\n# Push to Hub (optional)\nmodel.push_to_hub(\"username/my-lora-adapter\")\n</code></pre>"},{"location":"docs/finetuning/#2-setup-cactus","title":"2. Setup Cactus","text":"<p><pre><code>git clone https://github.com/cactus-compute/cactus &amp;&amp; cd cactus &amp;&amp; source ./setup\n</code></pre> </p>"},{"location":"docs/finetuning/#3-convert-for-cactus","title":"3. Convert for Cactus","text":"<p><pre><code># From local adapter: Use the correct base model!\ncactus convert Qwen/Qwen3-0.6B ./my-qwen3-0.6b --lora ./my-lora-adapter \n\n# From HuggingFace Hub: Use the correct base model!\ncactus convert Qwen/Qwen3-0.6B ./my-qwen3-0.6b  --lora username/my-lora-adapter \n</code></pre> </p>"},{"location":"docs/finetuning/#4-run","title":"4. Run","text":"<p>Test your model on Mac:</p> <p><pre><code>cactus run ./my-qwen3-0.6b\n</code></pre> </p>"},{"location":"docs/finetuning/#5-use-in-iosmacos-app","title":"5. Use in iOS/macOS App","text":"<p>Build the native library:</p> <p><pre><code>cactus build --apple\n</code></pre> <pre><code>Build complete!\nTotal time: 58 seconds\nStatic libraries:\n  Device: /Users/henry/Desktop/cactus/apple/libcactus-device.a\n  Simulator: /Users/henry/Desktop/cactus/apple/libcactus-simulator.a\nXCFrameworks:\n  iOS: /Users/henry/Desktop/cactus/apple/cactus-ios.xcframework\n  macOS: /Users/henry/Desktop/cactus/apple/cactus-macos.xcframework\nApple build complete!\n(venv) henry@Henrys-MacBook-Air cactus % \n</code></pre></p> <p>Link <code>cactus-ios.xcframework</code> to your Xcode project, then:</p> <p><pre><code>import Foundation\n\n// Load model from app bundle\nlet modelPath = Bundle.main.path(forResource: \"my-model\", ofType: nil)!\nlet model = cactus_init(modelPath, nil)\n\n// Run completion\nlet messages = \"[{\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"Hello!\\\"}]\"\nvar response = [CChar](repeating: 0, count: 4096)\ncactus_complete(model, messages, &amp;response, response.count, nil, nil, nil, nil)\nprint(String(cString: response))\n\n// Cleanup\ncactus_destroy(model)\n</code></pre> </p> <p>You can now build iOS apps using the following code,  but to see performance on any device while testing, run cactus tests by plugging any iphone to your Mac then running:</p> <pre><code>cactus test --&lt;model-path-or-name&gt; --ios \n</code></pre> <p>Cactus demo apps will eventually expand to using your custom fine-tunes. Also, <code>cactus run</code> will allow plugging in a phone,  such that the interactive session use the phone chips, this way you can test before fully building out your apps.</p>"},{"location":"docs/finetuning/#6-use-in-android-app","title":"6. Use in Android App","text":"<p>Build the native library:</p> <p><pre><code>cactus build --android\n</code></pre> <pre><code>Build complete!\nShared library location: /Users/henry/Desktop/cactus/android/libcactus.so\nStatic library location: /Users/henry/Desktop/cactus/android/libcactus.a\nAndroid build complete!\n(venv) henry@Henrys-MacBook-Air cactus % \n</code></pre></p> <p>Copy <code>libcactus.so</code> to <code>app/src/main/jniLibs/arm64-v8a/</code>, then:</p> <pre><code>class CactusWrapper {\n    init { System.loadLibrary(\"cactus\") }\n\n    external fun init(modelPath: String, contextSize: Long, corpusDir: String?): Long\n    external fun complete(model: Long, messagesJson: String, bufferSize: Int): String\n    external fun destroy(model: Long)\n}\n\n// Usage\nval cactus = CactusWrapper()\nval model = cactus.init(\"/data/local/tmp/my-model\", 2048, null)\nval response = cactus.complete(model, \"\"\"[{\"role\":\"user\",\"content\":\"Hello!\"}]\"\"\", 4096)\ncactus.destroy(model)\n</code></pre> <p>You can now build ANdroid apps using the following code,  but to see performance on any device while testing, run cactus tests by plugging any android phone to your Mac then running:</p> <pre><code>cactus test --&lt;model-path-or-name&gt; --android \n</code></pre> <p>Cactus demo apps will eventually expand to using your custom fine-tunes. Also, <code>cactus run</code> will allow plugging in a phone,  such that the interactive session use the phone chips, this way you can test before fully building out your apps.</p>"},{"location":"docs/finetuning/#resources","title":"Resources","text":"<ul> <li>Supported Base Models: <code>Qwen3, Gemma3, LFM2, SmolLM2</code> </li> <li>Full API reference: Cactus Engine</li> <li>Learn more and report bugs: Cactus</li> </ul>"},{"location":"docs/finetuning/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 Full C API reference for inference, streaming, and tool calling</li> <li>Runtime Compatibility \u2014 Ensure your weights match your Cactus runtime version</li> <li>Python SDK \u2014 Use fine-tuned models from Python</li> <li>Swift SDK \u2014 Deploy fine-tuned models in iOS/macOS apps</li> <li>Kotlin/Android SDK \u2014 Deploy fine-tuned models in Android apps</li> </ul>"},{"location":"flutter/","title":"Cactus for Flutter","text":"<p>Run AI models on-device with dart:ffi direct bindings for iOS, macOS, and Android.</p>"},{"location":"flutter/#building","title":"Building","text":"<pre><code>cactus build --flutter\n</code></pre> <p>Build output:</p> File Platform <code>libcactus.so</code> Android (arm64-v8a) <code>cactus-ios.xcframework</code> iOS <code>cactus-macos.xcframework</code> macOS <p>see the main README.md for how to use CLI &amp; download weight</p>"},{"location":"flutter/#integration","title":"Integration","text":""},{"location":"flutter/#android","title":"Android","text":"<ol> <li>Copy <code>libcactus.so</code> to <code>android/app/src/main/jniLibs/arm64-v8a/</code></li> <li>Copy <code>cactus.dart</code> to your <code>lib/</code> folder</li> </ol>"},{"location":"flutter/#ios","title":"iOS","text":"<ol> <li>Copy <code>cactus-ios.xcframework</code> to your <code>ios/</code> folder</li> <li>Open <code>ios/Runner.xcworkspace</code> in Xcode</li> <li>Drag the xcframework into the project</li> <li>In Runner target &gt; General &gt; \"Frameworks, Libraries, and Embedded Content\", set to \"Embed &amp; Sign\"</li> <li>Copy <code>cactus.dart</code> to your <code>lib/</code> folder</li> </ol>"},{"location":"flutter/#macos","title":"macOS","text":"<ol> <li>Copy <code>cactus-macos.xcframework</code> to your <code>macos/</code> folder</li> <li>Open <code>macos/Runner.xcworkspace</code> in Xcode</li> <li>Drag the xcframework into the project</li> <li>In Runner target &gt; General &gt; \"Frameworks, Libraries, and Embedded Content\", set to \"Embed &amp; Sign\"</li> <li>Copy <code>cactus.dart</code> to your <code>lib/</code> folder</li> </ol>"},{"location":"flutter/#usage","title":"Usage","text":""},{"location":"flutter/#basic-completion","title":"Basic Completion","text":"<pre><code>import 'cactus.dart';\n\nfinal model = Cactus.create('/path/to/model.gguf');\nfinal result = model.complete('What is the capital of France?');\nprint(result.text);\nmodel.dispose();\n</code></pre>"},{"location":"flutter/#chat-messages","title":"Chat Messages","text":"<pre><code>final model = Cactus.create(modelPath);\nfinal result = model.completeMessages([\n  Message.system('You are a helpful assistant.'),\n  Message.user('What is 2 + 2?'),\n]);\nprint(result.text);\nmodel.dispose();\n</code></pre>"},{"location":"flutter/#completion-options","title":"Completion Options","text":"<pre><code>final options = CompletionOptions(\n  temperature: 0.7,\n  topP: 0.9,\n  topK: 40,\n  maxTokens: 256,\n  stopSequences: ['\\n\\n'],\n);\n\nfinal result = model.complete('Write a haiku:', options: options);\n</code></pre>"},{"location":"flutter/#audio-transcription","title":"Audio Transcription","text":"<pre><code>// From file\nfinal result = model.transcribe('/path/to/audio.wav');\n\n// From PCM data (16kHz mono)\nfinal pcmData = Uint8List.fromList([...]); // Your PCM bytes\nfinal result = model.transcribePcm(pcmData);\n</code></pre>"},{"location":"flutter/#streaming-transcription","title":"Streaming Transcription","text":"<pre><code>final stream = model.createStreamTranscriber();\nstream.insert(audioChunk1);\nstream.insert(audioChunk2);\n\nfinal partial = stream.process();\nprint('Partial: ${partial.text}');\n\nfinal finalResult = stream.finalize();\nprint('Final: ${finalResult.text}');\n\nstream.dispose();\n</code></pre>"},{"location":"flutter/#embeddings","title":"Embeddings","text":"<pre><code>final embedding = model.embed('Hello, world!');\nfinal imageEmbedding = model.imageEmbed('/path/to/image.jpg');\nfinal audioEmbedding = model.audioEmbed('/path/to/audio.wav');\n</code></pre>"},{"location":"flutter/#tokenization","title":"Tokenization","text":"<pre><code>final tokens = model.tokenize('Hello, world!');\nfinal scores = model.scoreWindow(tokens, 0, tokens.length, 512);\n</code></pre>"},{"location":"flutter/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<pre><code>final model = Cactus.create(\n  '/path/to/model.gguf',\n  corpusDir: '/path/to/documents',\n);\n\nfinal result = model.complete('What does the documentation say about X?');\n</code></pre>"},{"location":"flutter/#vector-index","title":"Vector Index","text":"<pre><code>final index = CactusIndex.create('/path/to/index', embeddingDim: 384);\n\nindex.add(\n  ids: [1, 2],\n  documents: ['Document 1', 'Document 2'],\n  embeddings: [\n    model.embed('Document 1'),\n    model.embed('Document 2'),\n  ],\n);\n\nfinal results = index.query(model.embed('search query'), topK: 5);\nfor (final r in results) {\n  print('ID: ${r.id}, Score: ${r.score}');\n}\n\nindex.dispose();\n</code></pre>"},{"location":"flutter/#api-reference","title":"API Reference","text":""},{"location":"flutter/#cactus","title":"Cactus","text":"<pre><code>class Cactus {\n  static Cactus create(String modelPath, {String? corpusDir});\n\n  CompletionResult complete(String prompt, {CompletionOptions options, void Function(String, int)? onToken});\n  CompletionResult completeMessages(List&lt;Message&gt; messages, {CompletionOptions options, List&lt;Map&lt;String, dynamic&gt;&gt;? tools, void Function(String, int)? onToken});\n\n  TranscriptionResult transcribe(String audioPath, {String? prompt, TranscriptionOptions options});\n  TranscriptionResult transcribePcm(Uint8List pcmData, {String? prompt, TranscriptionOptions options});\n\n  List&lt;double&gt; embed(String text, {bool normalize = true});\n  List&lt;double&gt; imageEmbed(String imagePath);\n  List&lt;double&gt; audioEmbed(String audioPath);\n  String ragQuery(String query, {int topK = 5});\n\n  List&lt;int&gt; tokenize(String text);\n  String scoreWindow(List&lt;int&gt; tokens, int start, int end, int context);\n  StreamTranscriber createStreamTranscriber();\n\n  void reset();\n  void stop();\n  void dispose();\n\n  static String getLastError();\n}\n</code></pre>"},{"location":"flutter/#message","title":"Message","text":"<pre><code>class Message {\n  static Message system(String content);\n  static Message user(String content);\n  static Message assistant(String content);\n}\n</code></pre>"},{"location":"flutter/#completionoptions","title":"CompletionOptions","text":"<pre><code>class CompletionOptions {\n  final double temperature; \n  final double topP;  \n  final int topK;   \n  final int maxTokens;       \n  final List&lt;String&gt; stopSequences;\n  final double confidenceThreshold;\n\n  static const defaultOptions;\n}\n</code></pre>"},{"location":"flutter/#completionresult","title":"CompletionResult","text":"<pre><code>class CompletionResult {\n  final String text;\n  final List&lt;Map&lt;String, dynamic&gt;&gt;? functionCalls;\n  final int promptTokens;\n  final int completionTokens;\n  final double timeToFirstToken;\n  final double totalTime;\n  final double prefillTokensPerSecond;\n  final double decodeTokensPerSecond;\n  final double confidence;\n  final bool needsCloudHandoff;\n}\n</code></pre>"},{"location":"flutter/#transcriptionresult","title":"TranscriptionResult","text":"<pre><code>class TranscriptionResult {\n  final String text;\n  final List&lt;Map&lt;String, dynamic&gt;&gt;? segments;\n  final double totalTime;\n}\n</code></pre>"},{"location":"flutter/#streamtranscriber","title":"StreamTranscriber","text":"<pre><code>class StreamTranscriber {\n  void insert(Uint8List pcmData);\n  TranscriptionResult process({String? language});\n  TranscriptionResult finalize();\n  void dispose();\n}\n</code></pre>"},{"location":"flutter/#cactusindex","title":"CactusIndex","text":"<pre><code>class CactusIndex {\n  static CactusIndex create(String indexDir, {required int embeddingDim});\n\n  void add({required List&lt;int&gt; ids, required List&lt;String&gt; documents, required List&lt;List&lt;double&gt;&gt; embeddings, List&lt;String&gt;? metadatas});\n  void delete(List&lt;int&gt; ids);\n  List&lt;IndexResult&gt; query(List&lt;double&gt; embedding, {int topK = 5});\n  void compact();\n  void dispose();\n}\n\nclass IndexResult {\n  final int id;\n  final double score;\n}\n</code></pre>"},{"location":"flutter/#bundling-model-weights","title":"Bundling Model Weights","text":"<p>Models must be accessible via file path at runtime.</p>"},{"location":"flutter/#android_1","title":"Android","text":"<p>Copy from assets to internal storage on first launch:</p> <pre><code>import 'package:flutter/services.dart';\nimport 'package:path_provider/path_provider.dart';\nimport 'dart:io';\n\nFuture&lt;String&gt; getModelPath() async {\n  final dir = await getApplicationDocumentsDirectory();\n  final modelFile = File('${dir.path}/model.gguf');\n\n  if (!await modelFile.exists()) {\n    final data = await rootBundle.load('assets/model.gguf');\n    await modelFile.writeAsBytes(data.buffer.asUint8List());\n  }\n\n  return modelFile.path;\n}\n</code></pre>"},{"location":"flutter/#iosmacos","title":"iOS/macOS","text":"<p>Add model to bundle and access via path:</p> <pre><code>import 'package:path_provider/path_provider.dart';\n\nfinal path = '${Directory.current.path}/model.gguf';\n</code></pre>"},{"location":"flutter/#requirements","title":"Requirements","text":"<ul> <li>Flutter 3.0+</li> <li>Dart 2.17+</li> <li>iOS 14.0+ / macOS 13.0+</li> <li>Android API 24+ / arm64-v8a</li> </ul>"},{"location":"flutter/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 Full C API reference underlying the Flutter bindings</li> <li>Cactus Index API \u2014 Vector database API for RAG applications</li> <li>Fine-tuning Guide \u2014 Deploy custom fine-tunes to mobile</li> <li>Swift SDK \u2014 Native Swift alternative for Apple platforms</li> <li>Kotlin/Android SDK \u2014 Native Kotlin alternative for Android</li> </ul>"},{"location":"python/","title":"Cactus Python Package","text":"<p>Python bindings for Cactus Engine via FFI. Auto-installed when you run <code>source ./setup</code>.</p>"},{"location":"python/#getting-started","title":"Getting Started","text":"<pre><code># Setup environment\nsource ./setup\n\n# Build shared library for Python\ncactus build --python\n\n# Download models\ncactus download LiquidAI/LFM2-VL-450M\ncactus download openai/whisper-small\n\n# Optional: set your Cactus Cloud API key for automatic cloud fallback\ncactus auth\n</code></pre>"},{"location":"python/#quick-example","title":"Quick Example","text":"<pre><code>from cactus import CactusModel\nimport json\n\nwith CactusModel(\"weights/lfm2-vl-450m\") as model:\n    messages = [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n    response = json.loads(model.complete(messages))\n    print(response[\"response\"])\n</code></pre>"},{"location":"python/#api-reference","title":"API Reference","text":""},{"location":"python/#cactusmodelmodel_path-corpus_dirnone","title":"<code>CactusModel(model_path, corpus_dir=None)</code>","text":"<p>Initialize a model and act as a context manager.</p> Parameter Type Description <code>model_path</code> <code>str</code> Path to model weights directory <code>corpus_dir</code> <code>str</code> Optional path to RAG corpus directory for document Q&amp;A <pre><code>with CactusModel(\"weights/lfm2-vl-450m\") as model:\n    # use model...\n    pass\n\n# Or without context manager\nrag_model = CactusModel(\"weights/lfm2-rag\", corpus_dir=\"./documents\")\n# rag_model.destroy() must be called\n</code></pre>"},{"location":"python/#modelcompletemessages-options","title":"<code>model.complete(messages, **options)</code>","text":"<p>Run chat completion. Returns JSON string with response and metrics.</p> Parameter Type Description <code>messages</code> <code>list\\|str</code> List of message dicts or JSON string <code>tools</code> <code>list</code> Optional tool definitions for function calling <code>temperature</code> <code>float</code> Sampling temperature <code>top_p</code> <code>float</code> Top-p sampling <code>top_k</code> <code>int</code> Top-k sampling <code>max_tokens</code> <code>int</code> Maximum tokens to generate <code>stop_sequences</code> <code>list</code> Stop sequences <code>include_stop_sequences</code> <code>bool</code> Include matched stop sequences in output (default: <code>False</code>) <code>force_tools</code> <code>bool</code> Constrain output to tool call format <code>tool_rag_top_k</code> <code>int</code> Select top-k relevant tools via Tool RAG (default: 2, 0 = use all tools) <code>confidence_threshold</code> <code>float</code> Minimum confidence for local generation (default: 0.7, triggers cloud_handoff when below) <code>callback</code> <code>fn</code> Streaming callback <code>fn(token, token_id, user_data)</code> <pre><code># Basic completion\nwith CactusModel(\"weights/lfm2-vl-450m\") as model:\n    messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    response = model.complete(messages, max_tokens=100)\n    print(json.loads(response)[\"response\"])\n\n# With tools\ntools = [{\n    \"name\": \"get_weather\",\n    \"description\": \"Get weather for a location\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\"location\": {\"type\": \"string\"}},\n        \"required\": [\"location\"]\n    }\n}]\nwith CactusModel(\"weights/lfm2-vl-450m\") as model:\n    response = model.complete(messages, tools=tools)\n\n# Streaming\ndef on_token(token, token_id, user_data):\n    print(token, end=\"\", flush=True)\n\nwith CactusModel(\"weights/lfm2-vl-450m\") as model:\n    model.complete(messages, callback=on_token)\n</code></pre> <p>Response format (all fields always present): <pre><code>{\n    \"success\": true,\n    \"error\": null,\n    \"cloud_handoff\": false,\n    \"response\": \"Hello! How can I help?\",\n    \"function_calls\": [],\n    \"confidence\": 0.85,\n    \"time_to_first_token_ms\": 45.2,\n    \"total_time_ms\": 163.7,\n    \"prefill_tps\": 619.5,\n    \"decode_tps\": 168.4,\n    \"ram_usage_mb\": 245.67,\n    \"prefill_tokens\": 28,\n    \"decode_tokens\": 50,\n    \"total_tokens\": 78\n}\n</code></pre></p> <p>Cloud handoff response (when model detects low confidence): <pre><code>{\n    \"success\": false,\n    \"error\": null,\n    \"cloud_handoff\": true,\n    \"response\": null,\n    \"function_calls\": [],\n    \"confidence\": 0.18,\n    \"time_to_first_token_ms\": 45.2,\n    \"total_time_ms\": 45.2,\n    \"prefill_tps\": 619.5,\n    \"decode_tps\": 0.0,\n    \"ram_usage_mb\": 245.67,\n    \"prefill_tokens\": 28,\n    \"decode_tokens\": 0,\n    \"total_tokens\": 28\n}\n</code></pre></p> <p>When <code>cloud_handoff</code> is <code>True</code>, the model's confidence dropped below <code>confidence_threshold</code> (default: 0.7) and recommends deferring to a cloud-based model for better results. Handle this in your application:</p> <pre><code>with CactusModel(\"weights/lfm2-vl-450m\") as model:\n    result = json.loads(model.complete(messages))\n    if result[\"cloud_handoff\"]:\n        # Defer to cloud API (e.g., OpenAI, Anthropic)\n        response = call_cloud_api(messages)\n    else:\n        response = result[\"response\"]\n</code></pre>"},{"location":"python/#modeltranscribeaudio_path-prompt","title":"<code>model.transcribe(audio_path, prompt=\"\")</code>","text":"<p>Transcribe audio using a Whisper model. Returns JSON string.</p> Parameter Type Description <code>audio_path</code> <code>str</code> Path to audio file (WAV) <code>prompt</code> <code>str</code> Whisper prompt for language/task <pre><code>with CactusModel(\"weights/whisper-small\") as whisper:\n    prompt = \"&lt;|startoftranscript|&gt;&lt;|en|&gt;&lt;|transcribe|&gt;&lt;|notimestamps|&gt;\"\n    response = whisper.transcribe(\"audio.wav\", prompt=prompt)\n    print(json.loads(response)[\"response\"])\n</code></pre>"},{"location":"python/#modelembedtext-normalizefalse","title":"<code>model.embed(text, normalize=False)</code>","text":"<p>Get text embeddings. Returns list of floats.</p> Parameter Type Description <code>text</code> <code>str</code> Text to embed <code>normalize</code> <code>bool</code> L2-normalize embeddings (default: False) <pre><code>with CactusModel(\"weights/lfm2-vl-450m\") as model:\n    embedding = model.embed(\"Hello world\")\n    print(f\"Dimension: {len(embedding)}\")\n</code></pre>"},{"location":"python/#modelimage_embedimage_path","title":"<code>model.image_embed(image_path)</code>","text":"<p>Get image embeddings from a VLM. Returns list of floats.</p> <pre><code>with CactusModel(\"weights/lfm2-vl-450m\") as model:\n    embedding = model.image_embed(\"image.png\")\n</code></pre>"},{"location":"python/#modelaudio_embedaudio_path","title":"<code>model.audio_embed(audio_path)</code>","text":"<p>Get audio embeddings from a Whisper model. Returns list of floats.</p> <pre><code>with CactusModel(\"weights/whisper-small\") as whisper:\n    embedding = whisper.audio_embed(\"audio.wav\")\n</code></pre>"},{"location":"python/#modelreset","title":"<code>model.reset()</code>","text":"<p>Reset model state (clear KV cache). Call between unrelated conversations.</p> <pre><code>model.reset()\n</code></pre>"},{"location":"python/#modelstop","title":"<code>model.stop()</code>","text":"<p>Stop an ongoing generation (useful with streaming callbacks).</p> <pre><code>model.stop()\n</code></pre>"},{"location":"python/#modeltokenizetext","title":"<code>model.tokenize(text)</code>","text":"<p>Tokenize text. Returns list of token IDs.</p> <pre><code>with CactusModel(\"weights/lfm2-vl-450m\") as model:\n    tokens = model.tokenize(\"Hello world\")\n    print(tokens)  # [1234, 5678, ...]\n</code></pre>"},{"location":"python/#modelrag_queryquery-top_k5","title":"<code>model.rag_query(query, top_k=5)</code>","text":"<p>Query RAG corpus for relevant text chunks. Requires model initialized with <code>corpus_dir</code>.</p> Parameter Type Description <code>query</code> <code>str</code> Query text <code>top_k</code> <code>int</code> Number of chunks to retrieve (default: 5) <pre><code>with CactusModel(\"weights/lfm2-rag\", corpus_dir=\"./documents\") as model:\n    chunks = model.rag_query(\"What is machine learning?\", top_k=3)\n    for chunk in chunks:\n        print(f\"Score: {chunk['score']:.2f} - {chunk['text'][:100]}...\")\n</code></pre>"},{"location":"python/#vision-vlm","title":"Vision (VLM)","text":"<p>Pass images in the messages for vision-language models:</p> <pre><code>with CactusModel(\"weights/lfm2-vl-450m\") as vlm:\n    messages = [{\n        \"role\": \"user\",\n        \"content\": \"Describe this image\",\n        \"images\": [\"path/to/image.png\"]\n    }]\n    response = vlm.complete(messages)\n    print(json.loads(response)[\"response\"])\n</code></pre>"},{"location":"python/#full-example","title":"Full Example","text":"<p>See <code>python/example.py</code> for a complete example covering: - Text completion - Text/image/audio embeddings - Vision (VLM) - Speech transcription</p> <pre><code>python python/example.py\n</code></pre>"},{"location":"python/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 Full C API reference that the Python bindings wrap</li> <li>Cactus Index API \u2014 Vector database API for RAG applications</li> <li>Fine-tuning Guide \u2014 Train and deploy custom LoRA fine-tunes</li> <li>Runtime Compatibility \u2014 Weight versioning across releases</li> <li>Swift SDK \u2014 Swift bindings for iOS/macOS</li> <li>Kotlin/Android SDK \u2014 Kotlin bindings for Android</li> <li>Flutter SDK \u2014 Dart bindings for cross-platform mobile</li> </ul>"},{"location":"rust/","title":"Cactus Rust Bindings","text":"<p>Raw FFI bindings to the Cactus C API. Auto-generated via <code>bindgen</code>.</p>"},{"location":"rust/#installation","title":"Installation","text":"<p>Add to your <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\ncactus-sys = { path = \"rust/cactus-sys\" }\n</code></pre> <p>Build requirements: - CMake - C++20 compiler - On macOS: Xcode command line tools - On Linux: <code>build-essential</code>, <code>libcurl4-openssl-dev</code>, <code>libclang-dev</code></p>"},{"location":"rust/#usage","title":"Usage","text":"<p>All functions mirror the C API documented in <code>docs/cactus_engine.md</code>.</p> <p>For usage examples, see: - Test files: <code>rust/cactus-sys/tests/</code> - C API docs: <code>docs/cactus_engine.md</code> - Other SDKs: <code>python/README.md</code>, <code>apple/README.md</code></p>"},{"location":"rust/#testing","title":"Testing","text":"<pre><code>export CACTUS_MODEL_PATH=/path/to/model\nexport CACTUS_STT_MODEL_PATH=/path/to/whisper-model\ncargo test --manifest-path rust/Cargo.toml -- --nocapture\n</code></pre>"},{"location":"rust/#see-also","title":"See Also","text":"<ul> <li>Cactus Engine API \u2014 Full C API reference that the Rust bindings wrap</li> <li>Python SDK \u2014 Python bindings with higher-level wrappers</li> <li>Swift SDK \u2014 Swift bindings for Apple platforms</li> <li>Kotlin/Android SDK \u2014 Kotlin bindings for Android</li> </ul>"}]}